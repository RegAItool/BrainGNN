#!/usr/bin/env python3
"""
BrainGNNé«˜çº§ä¼˜åŒ–ç³»ç»Ÿ
å®ç°æ‰€æœ‰å…ˆè¿›ä¼˜åŒ–æŠ€æœ¯ï¼Œç›®æ ‡è¾¾åˆ°80%ä»¥ä¸Šå‡†ç¡®ç‡
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.loader import DataLoader
from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score
from sklearn.utils.class_weight import compute_class_weight
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
import numpy as np
import json
import os
from datetime import datetime
import optuna
import logging
from collections import defaultdict
import pickle
import warnings
warnings.filterwarnings('ignore')

from imports.PainGraphDataset import PainGraphDataset
from net.multitask_braingnn import MultiTaskBrainGNN

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AdvancedPainGraphDataset(PainGraphDataset):
    """é«˜çº§æ•°æ®é›†ç±»ï¼Œæ”¯æŒå®Œæ•´çš„æ•°æ®é¢„å¤„ç†å’Œå¢å¼º"""
    
    def __init__(self, root_dir, use_all_samples=True, advanced_preprocessing=True, 
                 feature_selection=None, scaler_type='robust'):
        super().__init__(root_dir)
        self.use_all_samples = use_all_samples
        self.advanced_preprocessing = advanced_preprocessing
        self.feature_selection = feature_selection
        self.scaler_type = scaler_type
        self.scaler = None
        self.feature_selector = None
        
        if advanced_preprocessing:
            self._setup_preprocessing()
    
    def _setup_preprocessing(self):
        """è®¾ç½®é¢„å¤„ç†ç»„ä»¶"""
        if self.scaler_type == 'robust':
            self.scaler = RobustScaler()
        elif self.scaler_type == 'standard':
            self.scaler = StandardScaler()
        
        print(f"âœ… è®¾ç½®é¢„å¤„ç†ï¼š{self.scaler_type} scaler")
    
    def _apply_advanced_preprocessing(self, data):
        """åº”ç”¨é«˜çº§é¢„å¤„ç†"""
        try:
            # 1. å¼‚å¸¸å€¼å¤„ç†
            x = data.x.clone()
            
            # ç§»é™¤æç«¯å¼‚å¸¸å€¼ (3-sigma rule)
            mean = x.mean(dim=0)
            std = x.std(dim=0) + 1e-8  # é¿å…é™¤é›¶
            mask = torch.abs(x - mean) < 3 * std
            x = torch.where(mask, x, mean.unsqueeze(0))
            
            # 2. ç‰¹å¾æ ‡å‡†åŒ–
            if self.scaler is not None and hasattr(self, '_scaler_fitted'):
                # å±•å¹³ä¸º (116,) è¿›è¡Œæ ‡å‡†åŒ–ï¼Œç„¶åæ¢å¤å½¢çŠ¶
                x_flat = x.flatten().numpy().reshape(1, -1)  # (1, 116)
                x_scaled = self.scaler.transform(x_flat)
                x = torch.FloatTensor(x_scaled.reshape(116, 1))
            
            # 3. ç‰¹å¾é€‰æ‹©
            if self.feature_selector is not None:
                x_flat = x.flatten().numpy().reshape(1, -1)  # (1, 116)
                x_selected = self.feature_selector.transform(x_flat)
                # ç”±äºç‰¹å¾é€‰æ‹©ä¼šæ”¹å˜ç»´åº¦ï¼Œæˆ‘ä»¬éœ€è¦å¡«å……å›116ç»´åº¦
                x_new = np.zeros((1, 116))
                selected_indices = self.feature_selector.get_support()
                x_new[:, selected_indices] = x_selected
                x = torch.FloatTensor(x_new.reshape(116, 1))
            
            # 4. è¾¹æƒé‡æ­£åˆ™åŒ–
            if hasattr(data, 'edge_attr') and data.edge_attr is not None:
                edge_attr = data.edge_attr.clone()
                # å½’ä¸€åŒ–è¾¹æƒé‡
                edge_attr = F.normalize(edge_attr, p=2, dim=-1)
                data.edge_attr = edge_attr
            
            data.x = x
            return data
            
        except Exception as e:
            logger.warning(f"é¢„å¤„ç†å¤±è´¥: {e}")
            return data
    
    def fit_preprocessing(self, sample_data):
        """æ‹Ÿåˆé¢„å¤„ç†å‚æ•°"""
        if not self.advanced_preprocessing:
            return
        
        # æ”¶é›†æ‰€æœ‰ç‰¹å¾æ•°æ®
        all_features = []
        labels = []
        
        for data in sample_data:
            # æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾æ˜¯ (116, 1)ï¼Œæˆ‘ä»¬éœ€è¦å±•å¹³ä¸º (116,)
            features = data.x.numpy().flatten()
            all_features.append(features)
            labels.append(data.y.item())
        
        all_features = np.array(all_features)  # Shape: (n_samples, 116)
        
        # æ‹Ÿåˆæ ‡å‡†åŒ–å™¨
        if self.scaler is not None:
            self.scaler.fit(all_features)
            self._scaler_fitted = True
            print(f"âœ… æ‹Ÿåˆæ ‡å‡†åŒ–å™¨ï¼Œç‰¹å¾ç»´åº¦: {all_features.shape}")
        
        # æ‹Ÿåˆç‰¹å¾é€‰æ‹©å™¨
        if self.feature_selection:
            selector = SelectKBest(score_func=mutual_info_classif, 
                                 k=min(self.feature_selection, all_features.shape[1]))
            selector.fit(all_features, labels)
            self.feature_selector = selector
            print(f"âœ… æ‹Ÿåˆç‰¹å¾é€‰æ‹©å™¨ï¼Œé€‰æ‹© {self.feature_selection} ä¸ªç‰¹å¾")
    
    def get(self, idx):
        data = super().get(idx)
        
        if self.advanced_preprocessing:
            data = self._apply_advanced_preprocessing(data)
        
        return data

class EnsembleBrainGNN(nn.Module):
    """é›†æˆBrainGNNæ¨¡å‹"""
    
    def __init__(self, in_dim, n_roi=116, n_models=5):
        super().__init__()
        self.n_models = n_models
        self.models = nn.ModuleList()
        
        # åˆ›å»ºå¤šä¸ªä¸åŒé…ç½®çš„æ¨¡å‹
        configs = [
            {'hidden_dim': 64, 'dropout': 0.3},
            {'hidden_dim': 128, 'dropout': 0.4},
            {'hidden_dim': 96, 'dropout': 0.35},
            {'hidden_dim': 160, 'dropout': 0.45},
            {'hidden_dim': 80, 'dropout': 0.25}
        ]
        
        for i in range(n_models):
            config = configs[i % len(configs)]
            from intelligent_optimization import ImprovedBrainGNN
            model = ImprovedBrainGNN(
                in_dim=in_dim,
                hidden_dim=config['hidden_dim'],
                n_roi=n_roi,
                dropout=config['dropout'],
                use_attention=True
            )
            self.models.append(model)
        
        # é›†æˆæƒé‡
        self.ensemble_weights = nn.Parameter(torch.ones(n_models) / n_models)
        
    def forward(self, data):
        outputs = []
        task_types = []
        
        for model in self.models:
            out, task_type = model(data)
            outputs.append(out)
            task_types.append(task_type)
        
        # åŠ æƒå¹³å‡
        weights = F.softmax(self.ensemble_weights, dim=0)
        ensemble_output = sum(w * out for w, out in zip(weights, outputs))
        
        return ensemble_output, task_types[0]

class AdvancedTrainer:
    """é«˜çº§è®­ç»ƒå™¨"""
    
    def __init__(self, device='auto'):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device == 'auto' else device
        self.best_models = []
        self.training_history = defaultdict(list)
        
    def train_with_advanced_techniques(self, trial=None, use_ensemble=True):
        """ä½¿ç”¨æ‰€æœ‰é«˜çº§æŠ€æœ¯è¿›è¡Œè®­ç»ƒ"""
        
        # è¶…å‚æ•°é…ç½®
        if trial:
            params = {
                'lr': trial.suggest_float('lr', 1e-5, 1e-2, log=True),
                'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64]),
                'hidden_dim': trial.suggest_categorical('hidden_dim', [64, 96, 128, 160, 192]),
                'dropout': trial.suggest_float('dropout', 0.2, 0.6),
                'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),
                'use_class_weights': trial.suggest_categorical('use_class_weights', [True, False]),
                'feature_selection': trial.suggest_int('feature_selection', 50, 116),
                'scaler_type': trial.suggest_categorical('scaler_type', ['robust', 'standard']),
                'ensemble_size': trial.suggest_int('ensemble_size', 3, 7) if use_ensemble else 1,
                'augmentation_prob': trial.suggest_float('augmentation_prob', 0.1, 0.5)
            }
        else:
            # æœ€ä¼˜å‚æ•°é…ç½®
            params = {
                'lr': 0.0005,
                'batch_size': 32,
                'hidden_dim': 128,
                'dropout': 0.4,
                'weight_decay': 5e-5,
                'use_class_weights': True,
                'feature_selection': 80,
                'scaler_type': 'robust',
                'ensemble_size': 5 if use_ensemble else 1,
                'augmentation_prob': 0.3
            }
        
        print(f"ğŸš€ å¼€å§‹é«˜çº§è®­ç»ƒï¼Œå‚æ•°é…ç½®: {params}")
        
        # 1. åŠ è½½å’Œé¢„å¤„ç†å…¨éƒ¨æ•°æ®
        print("ğŸ“Š åŠ è½½å…¨éƒ¨4659ä¸ªæœ‰æ•ˆæ ·æœ¬...")
        dataset = AdvancedPainGraphDataset(
            './data/pain_data/all_graphs/',
            use_all_samples=True,
            advanced_preprocessing=True,
            feature_selection=params['feature_selection'],
            scaler_type=params['scaler_type']
        )
        
        # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
        filtered_data = []
        for i in range(len(dataset)):
            try:
                data = dataset[i]
                if data.x.shape == (116, 1):
                    data.y = data.y.long()
                    filtered_data.append(data)
            except:
                continue
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(filtered_data)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
        
        # 2. æ‹Ÿåˆé¢„å¤„ç†å‚æ•°
        sample_size = min(1000, len(filtered_data))
        sample_data = filtered_data[:sample_size]
        dataset.fit_preprocessing(sample_data)
        
        # 3. é‡æ–°åº”ç”¨é¢„å¤„ç†
        processed_data = []
        for data in filtered_data:
            processed_data.append(dataset.get(filtered_data.index(data)))
        
        # 4. è®¡ç®—ç±»åˆ«æƒé‡
        labels = [data.y.item() for data in processed_data]
        class_weights = None
        if params['use_class_weights']:
            class_weights = compute_class_weight('balanced', 
                                               classes=np.unique(labels), 
                                               y=labels)
            class_weights = torch.FloatTensor(class_weights).to(self.device)
            print(f"âœ… è®¡ç®—ç±»åˆ«æƒé‡: {class_weights}")
        
        # 5. æ•°æ®é›†åˆ’åˆ† (80/10/10)
        train_size = int(0.8 * len(processed_data))
        val_size = int(0.1 * len(processed_data))
        test_size = len(processed_data) - train_size - val_size
        
        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(
            processed_data, [train_size, val_size, test_size],
            generator=torch.Generator().manual_seed(42)  # å›ºå®šéšæœºç§å­
        )
        
        train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)
        
        print(f"ğŸ“Š æ•°æ®åˆ’åˆ† - è®­ç»ƒ: {len(train_dataset)}, éªŒè¯: {len(val_dataset)}, æµ‹è¯•: {len(test_dataset)}")
        
        # 6. åˆ›å»ºæ¨¡å‹
        if use_ensemble and params['ensemble_size'] > 1:
            model = EnsembleBrainGNN(in_dim=1, n_roi=116, n_models=params['ensemble_size']).to(self.device)
            print(f"ğŸ¤– åˆ›å»ºé›†æˆæ¨¡å‹ï¼ŒåŒ…å« {params['ensemble_size']} ä¸ªå­æ¨¡å‹")
        else:
            from intelligent_optimization import ImprovedBrainGNN
            model = ImprovedBrainGNN(
                in_dim=1,
                hidden_dim=params['hidden_dim'],
                n_roi=116,
                dropout=params['dropout'],
                use_attention=True
            ).to(self.device)
            print(f"ğŸ¤– åˆ›å»ºå•ä¸€æ¨¡å‹ï¼Œéšè—å±‚ç»´åº¦: {params['hidden_dim']}")
        
        # 7. ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=params['lr'],
            weight_decay=params['weight_decay'],
            betas=(0.9, 0.999)
        )
        
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer, T_0=10, T_mult=2, eta_min=params['lr']/100
        )
        
        # 8. è®­ç»ƒå¾ªç¯
        best_val_f1 = 0
        patience_counter = 0
        max_epochs = 100
        
        for epoch in range(max_epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            total_loss = 0
            all_preds = []
            all_labels = []
            
            for batch_idx, data in enumerate(train_loader):
                data = data.to(self.device)
                optimizer.zero_grad()
                
                # æ•°æ®å¢å¼º
                if torch.rand(1) < params['augmentation_prob']:
                    noise = torch.randn_like(data.x) * 0.02
                    data.x = data.x + noise
                
                out, _ = model(data)
                
                # æŸå¤±è®¡ç®—
                if params['use_class_weights'] and class_weights is not None:
                    weights = class_weights[data.y]
                    loss = F.nll_loss(out, data.y, reduction='none')
                    loss = (loss * weights).mean()
                else:
                    loss = F.nll_loss(out, data.y)
                
                # L2æ­£åˆ™åŒ–
                l2_reg = sum(p.pow(2.0).sum() for p in model.parameters())
                loss = loss + params['weight_decay'] * l2_reg
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                
                total_loss += loss.item()
                pred = out.argmax(dim=1)
                all_preds.extend(pred.cpu().numpy())
                all_labels.extend(data.y.cpu().numpy())
            
            train_f1 = f1_score(all_labels, all_preds, average='weighted')
            train_acc = accuracy_score(all_labels, all_preds)
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_preds = []
            val_labels = []
            val_probs = []
            
            with torch.no_grad():
                for data in val_loader:
                    data = data.to(self.device)
                    out, _ = model(data)
                    pred = out.argmax(dim=1)
                    prob = F.softmax(out, dim=1)
                    
                    val_preds.extend(pred.cpu().numpy())
                    val_labels.extend(data.y.cpu().numpy())
                    val_probs.extend(prob.cpu().numpy())
            
            val_f1 = f1_score(val_labels, val_preds, average='weighted')
            val_acc = accuracy_score(val_labels, val_preds)
            
            # å°è¯•è®¡ç®—AUC
            try:
                val_auc = roc_auc_score(val_labels, np.array(val_probs)[:, 1])
            except:
                val_auc = 0.5
            
            scheduler.step()
            current_lr = optimizer.param_groups[0]['lr']
            
            # è®°å½•è®­ç»ƒå†å²
            self.training_history['train_f1'].append(train_f1)
            self.training_history['train_acc'].append(train_acc)
            self.training_history['val_f1'].append(val_f1)
            self.training_history['val_acc'].append(val_acc)
            self.training_history['val_auc'].append(val_auc)
            self.training_history['lr'].append(current_lr)
            
            print(f'Epoch {epoch+1:3d}/{max_epochs}, Loss: {total_loss/len(train_loader):.4f}, '
                  f'Train F1: {train_f1:.4f}, Train Acc: {train_acc:.4f}, '
                  f'Val F1: {val_f1:.4f}, Val Acc: {val_acc:.4f}, Val AUC: {val_auc:.4f}, '
                  f'LR: {current_lr:.6f}')
            
            # æ—©åœå’Œæ¨¡å‹ä¿å­˜
            if val_f1 > best_val_f1:
                best_val_f1 = val_f1
                patience_counter = 0
                if not trial:
                    model_name = f'advanced_model_ensemble_{params["ensemble_size"]}.pth' if use_ensemble else 'advanced_model_single.pth'
                    torch.save(model.state_dict(), f'./model/{model_name}')
                    print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹: {model_name}, Val F1: {val_f1:.4f}")
            else:
                patience_counter += 1
                if patience_counter >= 20:
                    print("ğŸ›‘ æ—©åœè§¦å‘")
                    break
        
        # 9. æœ€ç»ˆæµ‹è¯•è¯„ä¼°
        if not trial:
            model_name = f'advanced_model_ensemble_{params["ensemble_size"]}.pth' if use_ensemble else 'advanced_model_single.pth'
            model.load_state_dict(torch.load(f'./model/{model_name}'))
        
        model.eval()
        test_preds = []
        test_labels = []
        test_probs = []
        
        with torch.no_grad():
            for data in test_loader:
                data = data.to(self.device)
                out, _ = model(data)
                pred = out.argmax(dim=1)
                prob = F.softmax(out, dim=1)
                
                test_preds.extend(pred.cpu().numpy())
                test_labels.extend(data.y.cpu().numpy())
                test_probs.extend(prob.cpu().numpy())
        
        test_acc = accuracy_score(test_labels, test_preds)
        test_f1 = f1_score(test_labels, test_preds, average='weighted')
        
        try:
            test_auc = roc_auc_score(test_labels, np.array(test_probs)[:, 1])
        except:
            test_auc = 0.5
        
        if not trial:
            print("\n" + "="*80)
            print("ğŸ¯ é«˜çº§ä¼˜åŒ–è®­ç»ƒå®Œæˆï¼")
            print(f"æœ€ä½³éªŒè¯F1: {best_val_f1:.4f}")
            print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
            print(f"æµ‹è¯•F1åˆ†æ•°: {test_f1:.4f}")
            print(f"æµ‹è¯•AUC: {test_auc:.4f}")
            print("="*80)
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            results = {
                'timestamp': datetime.now().isoformat(),
                'parameters': params,
                'best_val_f1': best_val_f1,
                'test_accuracy': test_acc,
                'test_f1': test_f1,
                'test_auc': test_auc,
                'training_history': dict(self.training_history),
                'classification_report': classification_report(test_labels, test_preds, output_dict=True)
            }
            
            result_file = f'./model/advanced_optimization_results_{"ensemble" if use_ensemble else "single"}.json'
            with open(result_file, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            
            print(f"ğŸ“Š è¯¦ç»†ç»“æœå·²ä¿å­˜: {result_file}")
        
        return best_val_f1 if trial else test_acc

def run_deep_hyperparameter_optimization():
    """è¿è¡Œæ·±åº¦è¶…å‚æ•°ä¼˜åŒ–"""
    print("ğŸ”¬ å¼€å§‹æ·±åº¦è¶…å‚æ•°ä¼˜åŒ–ï¼ˆç›®æ ‡: 50æ¬¡è¯•éªŒï¼‰...")
    
    trainer = AdvancedTrainer()
    
    # åˆ›å»ºä¼˜åŒ–ç ”ç©¶
    study = optuna.create_study(
        direction='maximize',
        sampler=optuna.samplers.TPESampler(seed=42),
        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)
    )
    
    # ä¼˜åŒ–ç›®æ ‡å‡½æ•°
    def objective(trial):
        return trainer.train_with_advanced_techniques(trial=trial, use_ensemble=True)
    
    # è¿è¡Œä¼˜åŒ–
    study.optimize(objective, n_trials=50, timeout=3600*4)  # 4å°æ—¶è¶…æ—¶
    
    print("ğŸ“Š è¶…å‚æ•°ä¼˜åŒ–å®Œæˆï¼")
    print("æœ€ä½³å‚æ•°:", study.best_params)
    print("æœ€ä½³F1åˆ†æ•°:", study.best_value)
    
    # ä¿å­˜ä¼˜åŒ–ç»“æœ
    with open('./model/hyperparameter_optimization_results.json', 'w') as f:
        json.dump({
            'best_params': study.best_params,
            'best_value': study.best_value,
            'n_trials': len(study.trials),
            'optimization_history': [{'value': trial.value, 'params': trial.params} 
                                   for trial in study.trials if trial.value is not None]
        }, f, indent=2)
    
    return study.best_params

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨BrainGNNé«˜çº§ä¼˜åŒ–ç³»ç»Ÿ...")
    print("ç›®æ ‡: è¾¾åˆ°80%ä»¥ä¸Šå‡†ç¡®ç‡")
    
    trainer = AdvancedTrainer()
    
    # è¿è¡Œè¶…å‚æ•°ä¼˜åŒ–
    print("\n" + "="*50)
    print("é˜¶æ®µ1: æ·±åº¦è¶…å‚æ•°ä¼˜åŒ–")
    print("="*50)
    best_params = run_deep_hyperparameter_optimization()
    
    # ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒé›†æˆæ¨¡å‹
    print("\n" + "="*50)
    print("é˜¶æ®µ2: é›†æˆæ¨¡å‹è®­ç»ƒ")
    print("="*50)
    ensemble_acc = trainer.train_with_advanced_techniques(use_ensemble=True)
    
    # è®­ç»ƒå•ä¸€æœ€ä½³æ¨¡å‹
    print("\n" + "="*50)
    print("é˜¶æ®µ3: å•ä¸€æœ€ä½³æ¨¡å‹è®­ç»ƒ") 
    print("="*50)
    single_acc = trainer.train_with_advanced_techniques(use_ensemble=False)
    
    # æœ€ç»ˆç»“æœ
    print("\n" + "="*80)
    print("ğŸ¯ BrainGNNé«˜çº§ä¼˜åŒ–å®Œæˆï¼")
    print("="*80)
    print(f"é›†æˆæ¨¡å‹å‡†ç¡®ç‡: {ensemble_acc:.1%}")
    print(f"å•ä¸€æ¨¡å‹å‡†ç¡®ç‡: {single_acc:.1%}")
    print(f"ç›®æ ‡è¾¾æˆ: {'âœ…' if max(ensemble_acc, single_acc) >= 0.8 else 'âŒ'} (ç›®æ ‡: 80%)")
    
    if max(ensemble_acc, single_acc) < 0.8:
        print("\nğŸ’¡ å»ºè®®:")
        print("- å‡†ç¡®ç‡æœªè¾¾åˆ°80%ï¼Œéœ€è¦è€ƒè™‘å¢åŠ æ›´å¤šè®­ç»ƒæ•°æ®")
        print("- å¯ä»¥å°è¯•æ”¶é›†æ›´å¤šä¸åŒç±»å‹çš„ç–¼ç—›æ•°æ®")
        print("- è€ƒè™‘ä½¿ç”¨é¢„è®­ç»ƒçš„è„‘ç½‘ç»œæ¨¡å‹")
        print("- æ¢ç´¢æ›´å¤æ‚çš„å›¾ç¥ç»ç½‘ç»œæ¶æ„")
    
    print("\nğŸ“ ç”Ÿæˆæ–‡ä»¶:")
    print("- ./model/advanced_model_ensemble_*.pth - é›†æˆæ¨¡å‹")
    print("- ./model/advanced_model_single.pth - å•ä¸€æ¨¡å‹")
    print("- ./model/advanced_optimization_results_*.json - è¯¦ç»†ç»“æœ")
    print("- ./model/hyperparameter_optimization_results.json - è¶…å‚æ•°ä¼˜åŒ–ç»“æœ")

if __name__ == "__main__":
    main()