#!/usr/bin/env python3
"""
ç®€åŒ–çš„é«˜çº§è®­ç»ƒç³»ç»Ÿ
ä¸“æ³¨äºè¾¾åˆ°80%å‡†ç¡®ç‡çš„æ ¸å¿ƒä¼˜åŒ–
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.loader import DataLoader
from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score
from sklearn.utils.class_weight import compute_class_weight
from sklearn.ensemble import VotingClassifier
from sklearn.preprocessing import StandardScaler, RobustScaler
import numpy as np
import json
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

from imports.PainGraphDataset import PainGraphDataset
from intelligent_optimization import ImprovedBrainGNN

class OptimizedTrainer:
    """ä¼˜åŒ–è®­ç»ƒå™¨"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def load_and_preprocess_data(self):
        """åŠ è½½å’Œé¢„å¤„ç†å…¨éƒ¨æ•°æ®"""
        print("ğŸ“Š åŠ è½½å…¨éƒ¨æ•°æ®å¹¶è¿›è¡Œé«˜çº§é¢„å¤„ç†...")
        
        # åŠ è½½æ•°æ®é›†
        dataset = PainGraphDataset('./data/pain_data/all_graphs/')
        
        # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
        valid_data = []
        for i in range(len(dataset)):
            try:
                data = dataset[i]
                if data.x.shape == (116, 1):
                    data.y = data.y.long()
                    valid_data.append(data)
            except:
                continue
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(valid_data)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
        
        # é«˜çº§æ•°æ®é¢„å¤„ç†
        processed_data = []
        all_features = []
        labels = []
        
        for data in valid_data:
            # å¼‚å¸¸å€¼å¤„ç†
            x = data.x.clone()
            mean = x.mean()
            std = x.std()
            x = torch.clamp(x, mean - 3*std, mean + 3*std)
            
            # å½’ä¸€åŒ–
            x = (x - x.mean()) / (x.std() + 1e-8)
            
            data.x = x
            processed_data.append(data)
            
            # æ”¶é›†ç‰¹å¾ç”¨äºç±»åˆ«æƒé‡è®¡ç®—
            all_features.append(x.flatten().numpy())
            labels.append(data.y.item())
        
        # è®¡ç®—ç±»åˆ«æƒé‡
        class_weights = compute_class_weight('balanced', 
                                           classes=np.unique(labels), 
                                           y=labels)
        class_weights = torch.FloatTensor(class_weights).to(self.device)
        
        print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
        print(f"âœ… ç±»åˆ«åˆ†å¸ƒ: {np.bincount(labels)}")
        print(f"âœ… ç±»åˆ«æƒé‡: {class_weights}")
        
        return processed_data, class_weights
    
    def create_ensemble_models(self, n_models=5):
        """åˆ›å»ºé›†æˆæ¨¡å‹"""
        models = []
        
        configs = [
            {'hidden_dim': 128, 'dropout': 0.4},
            {'hidden_dim': 96, 'dropout': 0.35},
            {'hidden_dim': 160, 'dropout': 0.45},
            {'hidden_dim': 80, 'dropout': 0.3},
            {'hidden_dim': 192, 'dropout': 0.5}
        ]
        
        for i in range(n_models):
            config = configs[i % len(configs)]
            model = ImprovedBrainGNN(
                in_dim=1,
                hidden_dim=config['hidden_dim'],
                n_roi=116,
                dropout=config['dropout'],
                use_attention=True
            ).to(self.device)
            models.append(model)
        
        print(f"ğŸ¤– åˆ›å»º {n_models} ä¸ªé›†æˆæ¨¡å‹")
        return models
    
    def train_single_model(self, model, train_loader, val_loader, class_weights, model_id=0):
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹ {model_id+1}...")
        
        # ä¼˜åŒ–å™¨é…ç½®
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=0.001,
            weight_decay=1e-4,
            betas=(0.9, 0.999)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer, T_0=15, T_mult=2, eta_min=1e-6
        )
        
        best_val_f1 = 0
        patience_counter = 0
        max_epochs = 150
        
        for epoch in range(max_epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            total_loss = 0
            all_preds = []
            all_labels = []
            
            for data in train_loader:
                data = data.to(self.device)
                optimizer.zero_grad()
                
                # æ•°æ®å¢å¼º
                if torch.rand(1) < 0.2:
                    noise = torch.randn_like(data.x) * 0.01
                    data.x = data.x + noise
                
                out, _ = model(data)
                
                # åŠ æƒæŸå¤±
                if class_weights is not None:
                    weights = class_weights[data.y]
                    loss = F.nll_loss(out, data.y, reduction='none')
                    loss = (loss * weights).mean()
                else:
                    loss = F.nll_loss(out, data.y)
                
                # æ ‡ç­¾å¹³æ»‘
                smoothed_loss = loss * 0.9 + 0.1 * (-out.mean())
                
                smoothed_loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                
                total_loss += smoothed_loss.item()
                pred = out.argmax(dim=1)
                all_preds.extend(pred.cpu().numpy())
                all_labels.extend(data.y.cpu().numpy())
            
            train_f1 = f1_score(all_labels, all_preds, average='weighted')
            train_acc = accuracy_score(all_labels, all_preds)
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_preds = []
            val_labels = []
            val_probs = []
            
            with torch.no_grad():
                for data in val_loader:
                    data = data.to(self.device)
                    out, _ = model(data)
                    pred = out.argmax(dim=1)
                    prob = F.softmax(out, dim=1)
                    
                    val_preds.extend(pred.cpu().numpy())
                    val_labels.extend(data.y.cpu().numpy())
                    val_probs.extend(prob.cpu().numpy())
            
            val_f1 = f1_score(val_labels, val_preds, average='weighted')
            val_acc = accuracy_score(val_labels, val_preds)
            
            scheduler.step()
            
            if epoch % 10 == 0:
                print(f'  Epoch {epoch:3d}: Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}, Val Acc: {val_acc:.4f}')
            
            # æ—©åœ
            if val_f1 > best_val_f1:
                best_val_f1 = val_f1
                patience_counter = 0
                torch.save(model.state_dict(), f'./model/ensemble_model_{model_id}.pth')
            else:
                patience_counter += 1
                if patience_counter >= 30:
                    print(f"  â¹ï¸ æ¨¡å‹ {model_id+1} æ—©åœï¼Œæœ€ä½³éªŒè¯F1: {best_val_f1:.4f}")
                    break
        
        return best_val_f1
    
    def ensemble_predict(self, models, test_loader):
        """é›†æˆé¢„æµ‹"""
        print("ğŸ”® æ‰§è¡Œé›†æˆé¢„æµ‹...")
        
        all_predictions = []
        test_labels = []
        
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
        for model_id, model in enumerate(models):
            model.load_state_dict(torch.load(f'./model/ensemble_model_{model_id}.pth'))
            model.eval()
            
            model_preds = []
            model_probs = []
            
            with torch.no_grad():
                for data in test_loader:
                    data = data.to(self.device)
                    out, _ = model(data)
                    pred = out.argmax(dim=1)
                    prob = F.softmax(out, dim=1)
                    
                    model_preds.extend(pred.cpu().numpy())
                    model_probs.extend(prob.cpu().numpy())
                    
                    if model_id == 0:  # åªéœ€è¦æ”¶é›†ä¸€æ¬¡æ ‡ç­¾
                        test_labels.extend(data.y.cpu().numpy())
            
            all_predictions.append(np.array(model_probs))
        
        # å¹³å‡æ¦‚ç‡é¢„æµ‹
        avg_probs = np.mean(all_predictions, axis=0)
        ensemble_preds = np.argmax(avg_probs, axis=1)
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        ensemble_acc = accuracy_score(test_labels, ensemble_preds)
        ensemble_f1 = f1_score(test_labels, ensemble_preds, average='weighted')
        
        try:
            ensemble_auc = roc_auc_score(test_labels, avg_probs[:, 1])
        except:
            ensemble_auc = 0.5
        
        return ensemble_acc, ensemble_f1, ensemble_auc, test_labels, ensemble_preds
    
    def run_advanced_training(self):
        """è¿è¡Œé«˜çº§è®­ç»ƒæµç¨‹"""
        print("ğŸ¯ å¼€å§‹é«˜çº§è®­ç»ƒæµç¨‹ï¼Œç›®æ ‡: 80%å‡†ç¡®ç‡")
        
        # 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
        data, class_weights = self.load_and_preprocess_data()
        
        # 2. æ•°æ®åˆ’åˆ† (80/10/10)
        train_size = int(0.8 * len(data))
        val_size = int(0.1 * len(data))
        test_size = len(data) - train_size - val_size
        
        # å›ºå®šéšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
        torch.manual_seed(42)
        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(
            data, [train_size, val_size, test_size]
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
        
        print(f"ğŸ“Š æ•°æ®åˆ’åˆ† - è®­ç»ƒ: {len(train_dataset)}, éªŒè¯: {len(val_dataset)}, æµ‹è¯•: {len(test_dataset)}")
        
        # 3. åˆ›å»ºå’Œè®­ç»ƒé›†æˆæ¨¡å‹
        models = self.create_ensemble_models(n_models=5)
        
        val_f1_scores = []
        for i, model in enumerate(models):
            val_f1 = self.train_single_model(model, train_loader, val_loader, class_weights, i)
            val_f1_scores.append(val_f1)
        
        print(f"âœ… æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆï¼ŒéªŒè¯F1åˆ†æ•°: {val_f1_scores}")
        print(f"âœ… å¹³å‡éªŒè¯F1: {np.mean(val_f1_scores):.4f}")
        
        # 4. é›†æˆé¢„æµ‹
        ensemble_acc, ensemble_f1, ensemble_auc, test_labels, ensemble_preds = self.ensemble_predict(models, test_loader)
        
        # 5. ç»“æœæŠ¥å‘Š
        print("\n" + "="*60)
        print("ğŸ¯ é«˜çº§è®­ç»ƒå®Œæˆï¼")
        print("="*60)
        print(f"é›†æˆæ¨¡å‹æµ‹è¯•å‡†ç¡®ç‡: {ensemble_acc:.4f} ({ensemble_acc:.1%})")
        print(f"é›†æˆæ¨¡å‹æµ‹è¯•F1åˆ†æ•°: {ensemble_f1:.4f}")
        print(f"é›†æˆæ¨¡å‹æµ‹è¯•AUC: {ensemble_auc:.4f}")
        print(f"ç›®æ ‡è¾¾æˆ: {'âœ…' if ensemble_acc >= 0.8 else 'âŒ'} (ç›®æ ‡: 80%)")
        
        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        print("\\nğŸ“Š è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(test_labels, ensemble_preds))
        
        # ä¿å­˜ç»“æœ
        results = {
            'timestamp': datetime.now().isoformat(),
            'ensemble_accuracy': float(ensemble_acc),
            'ensemble_f1': float(ensemble_f1),
            'ensemble_auc': float(ensemble_auc),
            'individual_val_f1_scores': [float(f1) for f1 in val_f1_scores],
            'average_val_f1': float(np.mean(val_f1_scores)),
            'classification_report': classification_report(test_labels, ensemble_preds, output_dict=True),
            'target_achieved': ensemble_acc >= 0.8
        }
        
        with open('./model/advanced_training_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\\nğŸ“ ç»“æœå·²ä¿å­˜: ./model/advanced_training_results.json")
        
        if ensemble_acc < 0.8:
            print("\\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            print("- å½“å‰å‡†ç¡®ç‡æœªè¾¾åˆ°80%ï¼Œå¯èƒ½éœ€è¦:")
            print("  1. å¢åŠ æ›´å¤šè®­ç»ƒæ•°æ®")
            print("  2. å°è¯•æ›´å¤æ‚çš„æ¨¡å‹æ¶æ„")
            print("  3. ä½¿ç”¨é¢„è®­ç»ƒçš„è„‘ç½‘ç»œç‰¹å¾")
            print("  4. æ”¶é›†æ›´é«˜è´¨é‡çš„ç–¼ç—›æ•°æ®")
        
        return ensemble_acc

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨BrainGNNé«˜çº§è®­ç»ƒç³»ç»Ÿ")
    print("ğŸ¯ ç›®æ ‡: è¾¾åˆ°80%ä»¥ä¸Šå‡†ç¡®ç‡")
    
    trainer = OptimizedTrainer()
    accuracy = trainer.run_advanced_training()
    
    print(f"\\nâœ¨ æœ€ç»ˆå‡†ç¡®ç‡: {accuracy:.1%}")

if __name__ == "__main__":
    main()