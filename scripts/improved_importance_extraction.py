#!/usr/bin/env python3
"""
ÊîπËøõÁöÑROIÈáçË¶ÅÊÄßÂàÜÊï∞ÊèêÂèñ
‰ΩøÁî®Â§öÁßçÊñπÊ≥ïÊù•ËÆ°ÁÆóÊõ¥ÂáÜÁ°ÆÁöÑROIÈáçË¶ÅÊÄßÂàÜÊï∞
"""

import torch
import numpy as np
import os
import pickle
import argparse
from torch_geometric.data import DataLoader
from net.braingnn import Network
from imports.PainGraphDataset import PainGraphDataset
from imports.utils import train_val_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import glob
import shutil

def load_trained_model(model_path, args):
    """Âä†ËΩΩËÆ≠ÁªÉÂ•ΩÁöÑÊ®°Âûã"""
    model = Network(args.indim, args.ratio, args.nclass)
    
    if os.path.exists(model_path):
        checkpoint = torch.load(model_path, map_location='cpu')
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        print(f"‚úÖ ÊàêÂäüÂä†ËΩΩÊ®°Âûã: {model_path}")
        return model
    else:
        print(f"‚ùå Ê®°ÂûãÊñá‰ª∂‰∏çÂ≠òÂú®: {model_path}")
        return None

def extract_gradient_based_importance(model, dataloader, device='cpu'):
    """Âü∫‰∫éÊ¢ØÂ∫¶ÁöÑÈáçË¶ÅÊÄßÂàÜÊï∞ÊèêÂèñ"""
    print("üîç ÊèêÂèñÂü∫‰∫éÊ¢ØÂ∫¶ÁöÑÈáçË¶ÅÊÄßÂàÜÊï∞...")
    model.eval()
    gradient_importance = []
    
    for batch in dataloader:
        batch = batch.to(device)
        batch.x.requires_grad_(True)
        
        output, _, _, score1, score2 = model(batch.x, batch.edge_index, 
                                            batch.batch, batch.edge_attr, batch.pos)
        
        # ËÆ°ÁÆóÊçüÂ§±
        loss = torch.nn.functional.nll_loss(output, batch.y)
        loss.backward()
        
        # ËÆ°ÁÆóËæìÂÖ•ÁâπÂæÅÁöÑÊ¢ØÂ∫¶
        grad_x = batch.x.grad
        if grad_x is not None:
            # ‰ΩøÁî®Ê¢ØÂ∫¶ÁöÑÁªùÂØπÂÄº‰Ωú‰∏∫ÈáçË¶ÅÊÄß
            importance = torch.abs(grad_x).mean(dim=0)
            gradient_importance.append(importance.detach().cpu().numpy())
        
        batch.x.requires_grad_(False)
    
    if gradient_importance:
        return np.mean(gradient_importance, axis=0)
    else:
        return None

def extract_attention_based_importance(model, dataloader, device='cpu'):
    """Âü∫‰∫éÊ≥®ÊÑèÂäõÊùÉÈáçÁöÑÈáçË¶ÅÊÄßÂàÜÊï∞ÊèêÂèñ"""
    print("üîç ÊèêÂèñÂü∫‰∫éÊ≥®ÊÑèÂäõÊùÉÈáçÁöÑÈáçË¶ÅÊÄßÂàÜÊï∞...")
    model.eval()
    all_attention_scores = []
    
    with torch.no_grad():
        for batch in dataloader:
            batch = batch.to(device)
            
            # ÂâçÂêë‰º†Êí≠Ëé∑ÂèñpoolingÂàÜÊï∞
            output, _, _, score1, score2 = model(batch.x, batch.edge_index, 
                                                batch.batch, batch.edge_attr, batch.pos)
            
            # ‰ΩøÁî®Á¨¨‰∏ÄÂ±ÇÁöÑpoolingÂàÜÊï∞‰Ωú‰∏∫Ê≥®ÊÑèÂäõÊùÉÈáç
            attention_scores = torch.sigmoid(score1).mean(dim=0)
            all_attention_scores.append(attention_scores.cpu().numpy())
    
    return np.mean(all_attention_scores, axis=0)

def extract_statistical_importance(model, dataloader, device='cpu'):
    """Âü∫‰∫éÁªüËÆ°ÊñπÊ≥ïÁöÑÈáçË¶ÅÊÄßÂàÜÊï∞ÊèêÂèñ"""
    print("üîç ÊèêÂèñÂü∫‰∫éÁªüËÆ°ÊñπÊ≥ïÁöÑÈáçË¶ÅÊÄßÂàÜÊï∞...")
    model.eval()
    all_scores = []
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for batch in dataloader:
            batch = batch.to(device)
            
            output, _, _, score1, score2 = model(batch.x, batch.edge_index, 
                                                batch.batch, batch.edge_attr, batch.pos)
            
            all_scores.append(score1.cpu().numpy())
            pred = output.argmax(dim=1)
            all_predictions.append(pred.cpu().numpy())
            all_labels.append(batch.y.cpu().numpy())
    
    scores = np.concatenate(all_scores, axis=0)
    predictions = np.concatenate(all_predictions, axis=0)
    labels = np.concatenate(all_labels, axis=0)
    
    # Âè™‰ΩøÁî®Ê≠£Á°ÆÈ¢ÑÊµãÁöÑÊ†∑Êú¨
    correct_mask = (predictions == labels)
    
    if np.sum(correct_mask) > 0:
        # ËÆ°ÁÆóÊØè‰∏™ROIÁöÑÈáçË¶ÅÊÄß
        roi_importance = np.mean(scores[correct_mask], axis=0)
        
        # ËÆ°ÁÆóÁΩÆ‰ø°Â∫¶Âä†ÊùÉÁöÑÈáçË¶ÅÊÄß
        confidence = np.max(scores[correct_mask], axis=1)
        weights = confidence / np.sum(confidence)
        weighted_importance = np.average(scores[correct_mask], axis=0, weights=weights)
        
        # ËÆ°ÁÆóÊúÄÂ§ßÊøÄÊ¥ªÁöÑÈáçË¶ÅÊÄß
        max_importance = np.max(scores[correct_mask], axis=0)
        
        return roi_importance, weighted_importance, max_importance
    else:
        return None, None, None

def extract_feature_importance(model, dataloader, device='cpu'):
    """Âü∫‰∫éÁâπÂæÅÈáçË¶ÅÊÄßÂàÜÊûê"""
    print("üîç ÊèêÂèñÂü∫‰∫éÁâπÂæÅÈáçË¶ÅÊÄßÂàÜÊûê...")
    model.eval()
    feature_importance = []
    
    with torch.no_grad():
        for batch in dataloader:
            batch = batch.to(device)
            
            # Ëé∑Âèñ‰∏≠Èó¥Â±ÇÁâπÂæÅ
            output, _, _, score1, score2 = model(batch.x, batch.edge_index, 
                                                batch.batch, batch.edge_attr, batch.pos)
            
            # ‰ΩøÁî®ËæìÂÖ•ÁâπÂæÅÁöÑÊñπÂ∑Æ‰Ωú‰∏∫ÈáçË¶ÅÊÄßÊåáÊ†á
            feature_var = torch.var(batch.x, dim=0)
            feature_importance.append(feature_var.cpu().numpy())
    
    return np.mean(feature_importance, axis=0)

def calculate_ensemble_importance(methods_results):
    """ÈõÜÊàêÂ§öÁßçÊñπÊ≥ïÁöÑÈáçË¶ÅÊÄßÂàÜÊï∞"""
    print("üîç ËÆ°ÁÆóÈõÜÊàêÈáçË¶ÅÊÄßÂàÜÊï∞...")

    valid_results = []
    shapes = []
    for method_name, result in methods_results.items():
        if result is not None:
            if isinstance(result, tuple):
                arr = result[0]
            else:
                arr = result
            valid_results.append(arr)
            shapes.append(arr.shape)

    # Âè™‰øùÁïô shape ÂÆåÂÖ®‰∏ÄËá¥ÁöÑÁªìÊûú
    shape_counts = Counter(shapes)
    if not shape_counts:
        print("[Ë≠¶Âëä] Ê≤°ÊúâÂèØÁî®ÁöÑÁªìÊûúÁî®‰∫éÈõÜÊàêÔºÅ")
        return None
    # ÂèñÂá∫Áé∞Ê¨°Êï∞ÊúÄÂ§öÁöÑ shape
    target_shape = shape_counts.most_common(1)[0][0]
    filtered_results = [arr for arr in valid_results if arr.shape == target_shape]

    if filtered_results:
        normalized_results = []
        for result in filtered_results:
            if result.std() > 0:
                normalized = (result - result.mean()) / result.std()
            else:
                normalized = result
            normalized_results.append(normalized)
        ensemble_importance = np.mean(normalized_results, axis=0)
        return ensemble_importance
    else:
        print("[Ë≠¶Âëä] Ê≤°Êúâ shape ‰∏ÄËá¥ÁöÑÁªìÊûúÁî®‰∫éÈõÜÊàêÔºÅ")
        return None

def save_improved_importance_scores(results, save_dir='./importance_scores_improved'):
    """‰øùÂ≠òÊîπËøõÁöÑÈáçË¶ÅÊÄßÂàÜÊï∞"""
    os.makedirs(save_dir, exist_ok=True)
    
    for method_name, importance in results.items():
        if importance is not None:
            np.save(os.path.join(save_dir, f'{method_name}_importance.npy'), importance)
            
            # ‰øùÂ≠òÁªüËÆ°‰ø°ÊÅØ
            stats = {
                'method': method_name,
                'shape': importance.shape,
                'mean': float(np.mean(importance)),
                'std': float(np.std(importance)),
                'min': float(np.min(importance)),
                'max': float(np.max(importance)),
                'median': float(np.median(importance))
            }
            
            with open(os.path.join(save_dir, f'{method_name}_stats.pkl'), 'wb') as f:
                pickle.dump(stats, f)
    
    print(f"üíæ ÊîπËøõÁöÑÈáçË¶ÅÊÄßÂàÜÊï∞Â∑≤‰øùÂ≠òÂà∞: {save_dir}")

def visualize_importance_comparison(results, save_dir='./importance_scores_improved'):
    """ÂèØËßÜÂåñ‰∏çÂêåÊñπÊ≥ïÁöÑÈáçË¶ÅÊÄßÂàÜÊï∞ÊØîËæÉ"""
    print("üìä ÂàõÂª∫ÈáçË¶ÅÊÄßÂàÜÊï∞ÊØîËæÉÂõæ...")
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. ‰∏çÂêåÊñπÊ≥ïÁöÑÂàÜÊï∞ÂàÜÂ∏É
    ax1 = axes[0, 0]
    for method_name, importance in results.items():
        if importance is not None:
            ax1.hist(importance, bins=30, alpha=0.6, label=method_name)
    ax1.set_xlabel('Importance Score')
    ax1.set_ylabel('Frequency')
    ax1.set_title('Distribution of ROI Importance Scores')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Ââç20‰∏™ÊúÄÈáçË¶ÅROIÁöÑÊØîËæÉ
    ax2 = axes[0, 1]
    top_indices = {}
    for method_name, importance in results.items():
        if importance is not None:
            top_indices[method_name] = np.argsort(importance)[-20:][::-1]
    
    # ËÆ°ÁÆóÈáçÂè†Â∫¶
    if len(top_indices) > 1:
        methods = list(top_indices.keys())
        overlap_matrix = np.zeros((len(methods), len(methods)))
        for i, method1 in enumerate(methods):
            for j, method2 in enumerate(methods):
                overlap = len(set(top_indices[method1][:10]) & set(top_indices[method2][:10]))
                overlap_matrix[i, j] = overlap
        
        im = ax2.imshow(overlap_matrix, cmap='Blues', aspect='auto')
        ax2.set_xticks(range(len(methods)))
        ax2.set_yticks(range(len(methods)))
        ax2.set_xticklabels(methods, rotation=45)
        ax2.set_yticklabels(methods)
        ax2.set_title('Top-10 ROI Overlap Between Methods')
        plt.colorbar(im, ax=ax2)
    
    # 3. ÈáçË¶ÅÊÄßÂàÜÊï∞ÊéíÂ∫è
    ax3 = axes[1, 0]
    for method_name, importance in results.items():
        if importance is not None:
            sorted_scores = np.sort(importance)[::-1]
            ax3.plot(range(1, len(sorted_scores) + 1), sorted_scores, 
                    label=method_name, linewidth=2)
    ax3.set_xlabel('ROI Rank')
    ax3.set_ylabel('Importance Score')
    ax3.set_title('ROI Importance Scores (Ranked)')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. ÁªüËÆ°‰ø°ÊÅØË°®Ê†º
    ax4 = axes[1, 1]
    ax4.axis('off')
    
    info_text = "Importance Score Statistics:\n\n"
    for method_name, importance in results.items():
        if importance is not None:
            info_text += f"{method_name}:\n"
            info_text += f"  Mean: {np.mean(importance):.4f}\n"
            info_text += f"  Std: {np.std(importance):.4f}\n"
            info_text += f"  Max: {np.max(importance):.4f}\n"
            info_text += f"  Min: {np.min(importance):.4f}\n\n"
    
    ax4.text(0.1, 0.9, info_text, transform=ax4.transAxes, fontsize=10,
             verticalalignment='top', fontfamily='monospace')
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'importance_comparison.png'), dpi=300, bbox_inches='tight')
    plt.show()

def main():
    parser = argparse.ArgumentParser(description='ÊîπËøõÁöÑROIÈáçË¶ÅÊÄßÂàÜÊï∞ÊèêÂèñ')
    parser.add_argument('--model_path', type=str, default='./model_improved/best_model_fold0.pth',
                       help='ËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãË∑ØÂæÑ')
    parser.add_argument('--data_path', type=str, default='./data/ABIDE_pcp/cpac/filt_noglobal',
                       help='Êï∞ÊçÆË∑ØÂæÑ')
    parser.add_argument('--save_dir', type=str, default='./importance_scores_improved',
                       help='‰øùÂ≠òÈáçË¶ÅÊÄßÂàÜÊï∞ÁöÑÁõÆÂΩï')
    parser.add_argument('--batch_size', type=int, default=64,
                       help='ÊâπÂ§ÑÁêÜÂ§ßÂ∞è')
    parser.add_argument('--device', type=str, default='cpu',
                       help='ËÆæÂ§á (cpu/cuda)')
    
    # Ê®°ÂûãÂèÇÊï∞
    parser.add_argument('--indim', type=int, default=200, help='ËæìÂÖ•Áª¥Â∫¶')
    parser.add_argument('--ratio', type=float, default=0.6, help='poolingÊØî‰æã')
    parser.add_argument('--nclass', type=int, default=2, help='Á±ªÂà´Êï∞')
    parser.add_argument('--k', type=int, default=8, help='k (community num)')
    parser.add_argument('--nroi', type=int, default=116, help='ROIÊï∞ (R)')
    
    args = parser.parse_args()

    # === Ëá™Âä®Êé®Êñ≠ indim, k, nroi ===
    if os.path.exists(args.model_path):
        checkpoint = torch.load(args.model_path, map_location='cpu')
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
        else:
            state_dict = checkpoint
        # Â∞ùËØï‰ªé Linear Â±ÇÊàñ conv Â±ÇÊùÉÈáçËá™Âä®Êé®Êñ≠ indim, k, nroi
        indim_auto = None
        k_auto = None
        nroi_auto = None
        # Êé®Êñ≠ indim
        for k, v in state_dict.items():
            if ('.weight' in k) and (len(v.shape) == 2):
                indim_auto = v.shape[1]
                break
        # Êé®Êñ≠ k Âíå nroi (R) ‰ªé n1.0.weight: [k, R]
        if 'n1.0.weight' in state_dict:
            k_auto = state_dict['n1.0.weight'].shape[0]
            nroi_auto = state_dict['n1.0.weight'].shape[1]
        # Ëá™Âä®‰øÆÊ≠£
        if indim_auto is not None:
            if hasattr(args, 'indim') and args.indim != indim_auto:
                print(f"[Ëá™Âä®‰øÆÊ≠£] Ê£ÄÊµãÂà∞ÊùÉÈáçÊñá‰ª∂ËæìÂÖ•ÁâπÂæÅÁª¥Â∫¶‰∏∫ {indim_auto}ÔºåË¶ÜÁõñÂëΩ‰ª§Ë°åÂèÇÊï∞ indim={args.indim}")
            args.indim = indim_auto
            print(f"[Ëá™Âä®Êé®Êñ≠] ‰ΩøÁî®ÊùÉÈáçÊñá‰ª∂Êé®Êñ≠ÁöÑËæìÂÖ•ÁâπÂæÅÁª¥Â∫¶ indim={args.indim}")
        if k_auto is not None:
            if hasattr(args, 'k') and args.k != k_auto:
                print(f"[Ëá™Âä®‰øÆÊ≠£] Ê£ÄÊµãÂà∞ÊùÉÈáçÊñá‰ª∂ k={k_auto}ÔºåË¶ÜÁõñÂëΩ‰ª§Ë°åÂèÇÊï∞ k={args.k}")
            args.k = k_auto
            print(f"[Ëá™Âä®Êé®Êñ≠] ‰ΩøÁî®ÊùÉÈáçÊñá‰ª∂Êé®Êñ≠ÁöÑ k={args.k}")
        if nroi_auto is not None:
            if hasattr(args, 'nroi') and args.nroi != nroi_auto:
                print(f"[Ëá™Âä®‰øÆÊ≠£] Ê£ÄÊµãÂà∞ÊùÉÈáçÊñá‰ª∂ nroi={nroi_auto}ÔºåË¶ÜÁõñÂëΩ‰ª§Ë°åÂèÇÊï∞ nroi={args.nroi}")
            args.nroi = nroi_auto
            print(f"[Ëá™Âä®Êé®Êñ≠] ‰ΩøÁî®ÊùÉÈáçÊñá‰ª∂Êé®Êñ≠ÁöÑ nroi={args.nroi}")
        if indim_auto is None or k_auto is None or nroi_auto is None:
            print("[Ë≠¶Âëä] Êú™ËÉΩ‰ªéÊùÉÈáçÊñá‰ª∂Ëá™Âä®Êé®Êñ≠ÂÖ®ÈÉ®ÂèÇÊï∞ÔºåËØ∑Á°Æ‰øù --indim --k --nroi ÂèÇÊï∞Ê≠£Á°ÆÔºÅ")
    else:
        print(f"[Ë≠¶Âëä] ÊùÉÈáçÊñá‰ª∂ {args.model_path} ‰∏çÂ≠òÂú®ÔºåÊó†Ê≥ïËá™Âä®Êé®Êñ≠ÂèÇÊï∞„ÄÇ")

    print("ÔøΩÔøΩ ÂºÄÂßãÊîπËøõÁöÑÈáçË¶ÅÊÄßÂàÜÊï∞ÊèêÂèñ...")
    
    # 1. Âä†ËΩΩÊï∞ÊçÆ
    print("üìÇ Âä†ËΩΩÊï∞ÊçÆ...")
    dataset = ABIDEDataset(args.data_path, 'ABIDE')
    dataset.data.y = dataset.data.y.squeeze()
    dataset.data.x[dataset.data.x == float('inf')] = 0
    
    # Ëé∑ÂèñÊï∞ÊçÆÂàÜÂâ≤
    tr_index, val_index, te_index = train_val_test_split(fold=0)
    train_dataset = dataset[tr_index]
    val_dataset = dataset[val_index]
    test_dataset = dataset[te_index]
    
    # 2. ÂàõÂª∫Êï∞ÊçÆÂä†ËΩΩÂô®
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)
    
    # 3. Âä†ËΩΩÊ®°Âûã
    model = Network(args.indim, args.ratio, args.nclass, k=args.k, R=args.nroi)
    if os.path.exists(args.model_path):
        checkpoint = torch.load(args.model_path, map_location='cpu')
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        print(f"‚úÖ ÊàêÂäüÂä†ËΩΩÊ®°Âûã: {args.model_path}")
    else:
        print(f"‚ùå Ê®°ÂûãÊñá‰ª∂‰∏çÂ≠òÂú®: {args.model_path}")
        return
    model = model.to(args.device)
    
    # 4. ÊèêÂèñ‰∏çÂêåÊñπÊ≥ïÁöÑÈáçË¶ÅÊÄßÂàÜÊï∞
    results = {}
    
    # ÁªüËÆ°ÊñπÊ≥ï
    stat_importance, weighted_importance, max_importance = extract_statistical_importance(
        model, test_loader, args.device)
    results['statistical'] = stat_importance
    results['weighted'] = weighted_importance
    results['max_activation'] = max_importance
    
    # Ê≥®ÊÑèÂäõÊñπÊ≥ï
    attention_importance = extract_attention_based_importance(model, test_loader, args.device)
    results['attention'] = attention_importance
    
    # ÁâπÂæÅÈáçË¶ÅÊÄß
    feature_importance = extract_feature_importance(model, test_loader, args.device)
    results['feature'] = feature_importance
    
    # Ê¢ØÂ∫¶ÊñπÊ≥ï
    gradient_importance = extract_gradient_based_importance(model, test_loader, args.device)
    results['gradient'] = gradient_importance
    
    # 5. ËÆ°ÁÆóÈõÜÊàêÈáçË¶ÅÊÄß
    ensemble_importance = calculate_ensemble_importance(results)
    results['ensemble'] = ensemble_importance
    
    # 6. ‰øùÂ≠òÁªìÊûú
    save_improved_importance_scores(results, args.save_dir)
    
    # 7. ÂèØËßÜÂåñÊØîËæÉ
    visualize_importance_comparison(results, args.save_dir)
    
    print("‚úÖ ÊîπËøõÁöÑÈáçË¶ÅÊÄßÂàÜÊï∞ÊèêÂèñÂÆåÊàêÔºÅ")
    print(f"üìä ÊèêÂèñÁöÑÊñπÊ≥ï:")
    for method_name, importance in results.items():
        if importance is not None:
            print(f"   - {method_name}: ÂùáÂÄº={np.mean(importance):.4f}, Ê†áÂáÜÂ∑Æ={np.std(importance):.4f}")
    
    # Êé®Ëçê‰ΩøÁî®ÁöÑÊñπÊ≥ï
    if ensemble_importance is not None:
        print(f"üéØ Êé®Ëçê‰ΩøÁî®ÈõÜÊàêÊñπÊ≥ïÁöÑÈáçË¶ÅÊÄßÂàÜÊï∞")
        print(f"   Êñá‰ª∂‰ΩçÁΩÆ: {args.save_dir}/ensemble_importance.npy")

    # === Ëá™Âä®ËæìÂá∫ËÑëÂå∫ÂêçÁß∞Ë°® ===
    import json
    import pandas as pd
    from scipy.stats import spearmanr

    # === Ëá™Âä®ÁîüÊàê AAL116 ROI ÂêçÁß∞Êò†Â∞Ñ jsonÔºàÂ¶Ç‰∏çÂ≠òÂú®Ôºâ ===
    aal116_names = [
        "Precentral_L", "Precentral_R", "Frontal_Sup_L", "Frontal_Sup_R",
        "Frontal_Sup_Orb_L", "Frontal_Sup_Orb_R", "Frontal_Mid_L", "Frontal_Mid_R",
        "Frontal_Mid_Orb_L", "Frontal_Mid_Orb_R", "Frontal_Inf_Oper_L", "Frontal_Inf_Oper_R",
        "Frontal_Inf_Tri_L", "Frontal_Inf_Tri_R", "Frontal_Inf_Orb_L", "Frontal_Inf_Orb_R",
        "Rolandic_Oper_L", "Rolandic_Oper_R", "Supp_Motor_Area_L", "Supp_Motor_Area_R",
        "Olfactory_L", "Olfactory_R", "Frontal_Sup_Medial_L", "Frontal_Sup_Medial_R",
        "Frontal_Med_Orb_L", "Frontal_Med_Orb_R", "Rectus_L", "Rectus_R",
        "Insula_L", "Insula_R", "Cingulum_Ant_L", "Cingulum_Ant_R",
        "Cingulum_Mid_L", "Cingulum_Mid_R", "Cingulum_Post_L", "Cingulum_Post_R",
        "Hippocampus_L", "Hippocampus_R", "ParaHippocampal_L", "ParaHippocampal_R",
        "Amygdala_L", "Amygdala_R", "Calcarine_L", "Calcarine_R",
        "Cuneus_L", "Cuneus_R", "Lingual_L", "Lingual_R",
        "Occipital_Sup_L", "Occipital_Sup_R", "Occipital_Mid_L", "Occipital_Mid_R",
        "Occipital_Inf_L", "Occipital_Inf_R", "Fusiform_L", "Fusiform_R",
        "Postcentral_L", "Postcentral_R", "Parietal_Sup_L", "Parietal_Sup_R",
        "Parietal_Inf_L", "Parietal_Inf_R", "SupraMarginal_L", "SupraMarginal_R",
        "Angular_L", "Angular_R", "Precuneus_L", "Precuneus_R",
        "Paracentral_Lobule_L", "Paracentral_Lobule_R", "Caudate_L", "Caudate_R",
        "Putamen_L", "Putamen_R", "Pallidum_L", "Pallidum_R",
        "Thalamus_L", "Thalamus_R", "Heschl_L", "Heschl_R",
        "Temporal_Sup_L", "Temporal_Sup_R", "Temporal_Pole_Sup_L", "Temporal_Pole_Sup_R",
        "Temporal_Mid_L", "Temporal_Mid_R", "Temporal_Pole_Mid_L", "Temporal_Pole_Mid_R",
        "Temporal_Inf_L", "Temporal_Inf_R", "Cerebelum_Crus1_L", "Cerebelum_Crus1_R",
        "Cerebelum_Crus2_L", "Cerebelum_Crus2_R", "Cerebelum_3_L", "Cerebelum_3_R",
        "Cerebelum_4_5_L", "Cerebelum_4_5_R", "Cerebelum_6_L", "Cerebelum_6_R",
        "Cerebelum_7b_L", "Cerebelum_7b_R", "Cerebelum_8_L", "Cerebelum_8_R",
        "Cerebelum_9_L", "Cerebelum_9_R", "Cerebelum_10_L", "Cerebelum_10_R",
        "Vermis_1_2", "Vermis_3", "Vermis_4_5", "Vermis_6", "Vermis_7", "Vermis_8", "Vermis_9", "Vermis_10"
    ]
    aal116_json_path = './aal116_roi_id2name.json'
    if not os.path.exists(aal116_json_path):
        roi2name = {str(i): name for i, name in enumerate(aal116_names)}
        with open(aal116_json_path, 'w') as f:
            json.dump(roi2name, f, indent=2)
        print(f'Â∑≤Ëá™Âä®ÁîüÊàêÊ†áÂáÜAAL116ËÑëÂå∫ÂêçÁß∞Êò†Â∞Ñ: {aal116_json_path}')
    # Áî®AAL116Ê†áÂáÜÊò†Â∞Ñ
    roi_info_path = aal116_json_path

    ensemble_path = os.path.join(args.save_dir, 'ensemble_importance.npy')
    output_csv = os.path.join(args.save_dir, 'roi_importance_with_name.csv')
    if os.path.exists(ensemble_path) and os.path.exists(roi_info_path):
        importance = np.load(ensemble_path)
        with open(roi_info_path, 'r') as f:
            roi2name = json.load(f)
        df = pd.DataFrame({
            'ROI': np.arange(len(importance)),
            'BrainRegion': [roi2name.get(str(i), f'ROI_{i}') for i in range(len(importance))],
            'Importance': importance
        })
        df = df.sort_values('Importance', ascending=False)
        df['Rank'] = np.arange(1, len(df)+1)
        df.to_csv(output_csv, index=False)
        print(f'Â∑≤‰øùÂ≠ò: {output_csv}')
        print(df.head(10))

    # === Ëá™Âä®‰∏é‰∏¥Â∫äÊï∞ÊçÆÁõ∏ÂÖ≥ÊÄßÂàÜÊûê ===
    clinical_csv = './data/clinical_scores.csv'
    corr_csv = os.path.join(args.save_dir, 'roi_clinical_correlation.csv')
    if os.path.exists(ensemble_path) and os.path.exists(clinical_csv):
        importance = np.load(ensemble_path)
        clinical = pd.read_csv(clinical_csv)
        results = []
        for col in clinical.columns[1:]:
            corr, pval = spearmanr(importance, clinical[col])
            results.append({'ClinicalVar': col, 'SpearmanR': corr, 'PValue': pval})
        df_corr = pd.DataFrame(results).sort_values('PValue')
        df_corr.to_csv(corr_csv, index=False)
        print(f'Â∑≤‰øùÂ≠ò: {corr_csv}')
        print(df_corr.head(5))

    # === Ëá™Âä®ÁîüÊàêÂ§ö‰ªªÂä°ÂØπÊØîÁÉ≠Âõæ ===
    csv_files = glob.glob('./results/*/roi_importance_with_name.csv')
    if len(csv_files) >= 2:
        dfs = []
        task_names = []
        for path in csv_files:
            task = os.path.basename(os.path.dirname(path))
            df = pd.read_csv(path)[['BrainRegion', 'Importance']]
            df = df.rename(columns={'Importance': task})
            dfs.append(df.set_index('BrainRegion'))
            task_names.append(task)
        heat = pd.concat(dfs, axis=1).fillna(0)
        plt.figure(figsize=(8, max(10, len(heat)//3)))
        sns.heatmap(heat, cmap='viridis')
        plt.title('ROI Importance Across Tasks')
        plt.tight_layout()
        multitask_heatmap_path = './results/roi_importance_multitask_heatmap.png'
        plt.savefig(multitask_heatmap_path)
        print(f'Â∑≤‰øùÂ≠òÂ§ö‰ªªÂä°ÂØπÊØîÁÉ≠Âõæ: {multitask_heatmap_path}')

def extract_and_save_importance(args):
    print("üìÇ Âä†ËΩΩÊï∞ÊçÆ...")
    dataset = PainGraphDataset(args.data_path)
    from torch.utils.data import Subset
    import numpy as np
    import os
    all_task_types = set()
    for i in range(len(dataset)):
        data = dataset.get(i)
        if hasattr(data, 'task_type'):
            all_task_types.add(int(data.task_type[0].item()))
    print(f"Ê£ÄÊµãÂà∞‰ªªÂä°Á±ªÂûã: {sorted(all_task_types)}")

    # ÂÖàËØªÂèñÊ®°ÂûãÊùÉÈáçÔºåÊé®Êñ≠ÊúüÊúõÁöÑËæìÂÖ•ÁâπÂæÅÊï∞
    checkpoint_expected_input_dim = None
    if os.path.exists(args.model_path):
        checkpoint = torch.load(args.model_path, map_location='cpu')
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
        else:
            state_dict = checkpoint
        # Êé®Êñ≠ indim
        for k, v in state_dict.items():
            if ('.weight' in k) and (len(v.shape) == 2):
                checkpoint_expected_input_dim = v.shape[1]
                break
        print(f"Ê®°ÂûãÊùÉÈáçÊúüÊúõËæìÂÖ•ÁâπÂæÅÊï∞: {checkpoint_expected_input_dim}")
    else:
        print(f"‚ùå Ê®°ÂûãÊùÉÈáçÊñá‰ª∂ {args.model_path} ‰∏çÂ≠òÂú®ÔºåÊó†Ê≥ïÊé®Êñ≠ËæìÂÖ•ÁâπÂæÅÊï∞„ÄÇ")
        return

    for ttype in sorted(all_task_types):
        if ttype == -1:
            print(f"Ë∑≥ËøáÊó†Êïà task_type={ttype}")
            continue
        indices = [i for i in range(len(dataset)) if hasattr(dataset.get(i), 'task_type') and int(dataset.get(i).task_type[0].item()) == ttype]
        if not indices:
            continue
        print(f"\n==== Â§ÑÁêÜ task_type={ttype} ÁöÑÂ≠êÈõÜÔºåÂÖ± {len(indices)} ‰∏™Ê†∑Êú¨ ====")
        sub_dataset = Subset(dataset, indices)
        sample_data = dataset.get(indices[0])
        if checkpoint_expected_input_dim is not None and sample_data.x.size(1) != checkpoint_expected_input_dim:
            print(f"Ë∑≥Ëøá task_type={ttype}ÔºåÁâπÂæÅÊï∞ {sample_data.x.size(1)} ‰∏éÊùÉÈáçÊúüÊúõ {checkpoint_expected_input_dim} ‰∏ç‰∏ÄËá¥")
            continue
        from torch_geometric.loader import DataLoader
        test_loader = DataLoader(sub_dataset, batch_size=args.batch_size, shuffle=False)
        args.indim = sample_data.x.size(1)
        args.nroi = sample_data.x.size(0)
        model = Network(args.indim, args.ratio, args.nclass, k=args.k, R=args.nroi)
        if os.path.exists(args.model_path):
            checkpoint = torch.load(args.model_path, map_location='cpu')
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            else:
                model.load_state_dict(checkpoint)
            print(f"‚úÖ ÊàêÂäüÂä†ËΩΩÊ®°Âûã: {args.model_path}")
        else:
            print(f"‚ùå Ê®°ÂûãÊñá‰ª∂‰∏çÂ≠òÂú®: {args.model_path}")
            continue
        model = model.to(args.device)
        results = {}
        stat_importance, weighted_importance, max_importance = extract_statistical_importance(model, test_loader, args.device)
        results['statistical'] = stat_importance
        results['weighted'] = weighted_importance
        results['max_activation'] = max_importance
        attention_importance = extract_attention_based_importance(model, test_loader, args.device)
        results['attention'] = attention_importance
        feature_importance = extract_feature_importance(model, test_loader, args.device)
        results['feature'] = feature_importance
        gradient_importance = extract_gradient_based_importance(model, test_loader, args.device)
        results['gradient'] = gradient_importance
        ensemble_importance = calculate_ensemble_importance(results)
        results['ensemble'] = ensemble_importance
        save_dir = os.path.join(args.save_dir, f'task{ttype}')
        os.makedirs(save_dir, exist_ok=True)
        save_improved_importance_scores(results, save_dir)
        src_npy = os.path.join(save_dir, 'ensemble_importance.npy')
        dst_npy = os.path.join(save_dir, f'ensemble_importance_task{ttype}.npy')
        if os.path.exists(src_npy):
            import shutil
            shutil.copy(src_npy, dst_npy)
        src_csv = os.path.join(save_dir, 'roi_importance_with_name.csv')
        dst_csv = os.path.join(save_dir, f'roi_importance_task{ttype}.csv')
        if os.path.exists(src_csv):
            shutil.copy(src_csv, dst_csv)
        print(f"Â∑≤‰øùÂ≠ò task{ttype} ÁöÑÈáçË¶ÅÊÄßÁªìÊûúÂà∞ {save_dir}")

if __name__ == '__main__':
    # Âè™Â§ÑÁêÜ all_graphs ÁõÆÂΩï
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--device', type=str, default='cpu')
    parser.add_argument('--batch_size', type=int, default=64)
    parser.add_argument('--ratio', type=float, default=0.6)
    parser.add_argument('--nclass', type=int, default=2)
    parser.add_argument('--indim', type=int, default=200)
    parser.add_argument('--k', type=int, default=8)
    parser.add_argument('--nroi', type=int, default=116)
    parser.add_argument('--model_path', type=str, default='./model/0.pth')
    parser.add_argument('--save_dir', type=str, default='./results/all_graphs')
    args = parser.parse_args()
    args.data_path = 'data/pain_data/all_graphs'
    extract_and_save_importance(args) 