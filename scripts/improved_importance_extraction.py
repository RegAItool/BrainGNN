#!/usr/bin/env python3
"""
æ”¹è¿›çš„ROIé‡è¦æ€§åˆ†æ•°æå–
ä½¿ç”¨å¤šç§æ–¹æ³•æ¥è®¡ç®—æ›´å‡†ç¡®çš„ROIé‡è¦æ€§åˆ†æ•°
"""

import torch
import numpy as np
import os
import pickle
import argparse
from torch_geometric.data import DataLoader
from net.braingnn import Network
from imports.PainGraphDataset import PainGraphDataset
from imports.utils import train_val_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import glob
import shutil

def load_trained_model(model_path, args):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    model = Network(args.indim, args.ratio, args.nclass)
    
    if os.path.exists(model_path):
        checkpoint = torch.load(model_path, map_location='cpu')
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")
        return model
    else:
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return None

def extract_gradient_based_importance(model, dataloader, device='cpu'):
    """åŸºäºæ¢¯åº¦çš„é‡è¦æ€§åˆ†æ•°æå–"""
    print("ğŸ” æå–åŸºäºæ¢¯åº¦çš„é‡è¦æ€§åˆ†æ•°...")
    model.eval()
    gradient_importance = []
    
    for batch in dataloader:
        batch = batch.to(device)
        batch.x.requires_grad_(True)
        
        output, _, _, score1, score2 = model(batch.x, batch.edge_index, 
                                            batch.batch, batch.edge_attr, batch.pos)
        
        # è®¡ç®—æŸå¤±
        loss = torch.nn.functional.nll_loss(output, batch.y)
        loss.backward()
        
        # è®¡ç®—è¾“å…¥ç‰¹å¾çš„æ¢¯åº¦
        grad_x = batch.x.grad
        if grad_x is not None:
            # ä½¿ç”¨æ¢¯åº¦çš„ç»å¯¹å€¼ä½œä¸ºé‡è¦æ€§
            importance = torch.abs(grad_x).mean(dim=0)
            gradient_importance.append(importance.detach().cpu().numpy())
        
        batch.x.requires_grad_(False)
    
    if gradient_importance:
        return np.mean(gradient_importance, axis=0)
    else:
        return None

def extract_attention_based_importance(model, dataloader, device='cpu'):
    """åŸºäºæ³¨æ„åŠ›æƒé‡çš„é‡è¦æ€§åˆ†æ•°æå–"""
    print("ğŸ” æå–åŸºäºæ³¨æ„åŠ›æƒé‡çš„é‡è¦æ€§åˆ†æ•°...")
    model.eval()
    all_attention_scores = []
    
    with torch.no_grad():
        for batch in dataloader:
            batch = batch.to(device)
            
            # å‰å‘ä¼ æ’­è·å–poolingåˆ†æ•°
            output, _, _, score1, score2 = model(batch.x, batch.edge_index, 
                                                batch.batch, batch.edge_attr, batch.pos)
            
            # ä½¿ç”¨ç¬¬ä¸€å±‚çš„poolingåˆ†æ•°ä½œä¸ºæ³¨æ„åŠ›æƒé‡
            attention_scores = torch.sigmoid(score1).mean(dim=0)
            all_attention_scores.append(attention_scores.cpu().numpy())
    
    return np.mean(all_attention_scores, axis=0)

def extract_statistical_importance(model, dataloader, device='cpu'):
    """åŸºäºç»Ÿè®¡æ–¹æ³•çš„é‡è¦æ€§åˆ†æ•°æå–"""
    print("ğŸ” æå–åŸºäºç»Ÿè®¡æ–¹æ³•çš„é‡è¦æ€§åˆ†æ•°...")
    model.eval()
    all_scores = []
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for batch in dataloader:
            batch = batch.to(device)
            
            output, _, _, score1, score2 = model(batch.x, batch.edge_index, 
                                                batch.batch, batch.edge_attr, batch.pos)
            
            all_scores.append(score1.cpu().numpy())
            pred = output.argmax(dim=1)
            all_predictions.append(pred.cpu().numpy())
            all_labels.append(batch.y.cpu().numpy())
    
    scores = np.concatenate(all_scores, axis=0)
    predictions = np.concatenate(all_predictions, axis=0)
    labels = np.concatenate(all_labels, axis=0)
    
    # åªä½¿ç”¨æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬
    correct_mask = (predictions == labels)
    
    if np.sum(correct_mask) > 0:
        # è®¡ç®—æ¯ä¸ªROIçš„é‡è¦æ€§
        roi_importance = np.mean(scores[correct_mask], axis=0)
        
        # è®¡ç®—ç½®ä¿¡åº¦åŠ æƒçš„é‡è¦æ€§
        confidence = np.max(scores[correct_mask], axis=1)
        weights = confidence / np.sum(confidence)
        weighted_importance = np.average(scores[correct_mask], axis=0, weights=weights)
        
        # è®¡ç®—æœ€å¤§æ¿€æ´»çš„é‡è¦æ€§
        max_importance = np.max(scores[correct_mask], axis=0)
        
        return roi_importance, weighted_importance, max_importance
    else:
        return None, None, None

def extract_feature_importance(model, dataloader, device='cpu'):
    """åŸºäºç‰¹å¾é‡è¦æ€§åˆ†æ"""
    print("ğŸ” æå–åŸºäºç‰¹å¾é‡è¦æ€§åˆ†æ...")
    model.eval()
    feature_importance = []
    
    with torch.no_grad():
        for batch in dataloader:
            batch = batch.to(device)
            
            # è·å–ä¸­é—´å±‚ç‰¹å¾
            output, _, _, score1, score2 = model(batch.x, batch.edge_index, 
                                                batch.batch, batch.edge_attr, batch.pos)
            
            # ä½¿ç”¨è¾“å…¥ç‰¹å¾çš„æ–¹å·®ä½œä¸ºé‡è¦æ€§æŒ‡æ ‡
            feature_var = torch.var(batch.x, dim=0)
            feature_importance.append(feature_var.cpu().numpy())
    
    return np.mean(feature_importance, axis=0)

def calculate_ensemble_importance(methods_results):
    """é›†æˆå¤šç§æ–¹æ³•çš„é‡è¦æ€§åˆ†æ•°"""
    print("ğŸ” è®¡ç®—é›†æˆé‡è¦æ€§åˆ†æ•°...")

    valid_results = []
    shapes = []
    for method_name, result in methods_results.items():
        if result is not None:
            if isinstance(result, tuple):
                arr = result[0]
            else:
                arr = result
            valid_results.append(arr)
            shapes.append(arr.shape)

    # åªä¿ç•™ shape å®Œå…¨ä¸€è‡´çš„ç»“æœ
    shape_counts = Counter(shapes)
    if not shape_counts:
        print("[è­¦å‘Š] æ²¡æœ‰å¯ç”¨çš„ç»“æœç”¨äºé›†æˆï¼")
        return None
    # å–å‡ºç°æ¬¡æ•°æœ€å¤šçš„ shape
    target_shape = shape_counts.most_common(1)[0][0]
    filtered_results = [arr for arr in valid_results if arr.shape == target_shape]

    if filtered_results:
        normalized_results = []
        for result in filtered_results:
            if result.std() > 0:
                normalized = (result - result.mean()) / result.std()
            else:
                normalized = result
            normalized_results.append(normalized)
        ensemble_importance = np.mean(normalized_results, axis=0)
        return ensemble_importance
    else:
        print("[è­¦å‘Š] æ²¡æœ‰ shape ä¸€è‡´çš„ç»“æœç”¨äºé›†æˆï¼")
        return None

def save_improved_importance_scores(results, save_dir='./importance_scores_improved'):
    """ä¿å­˜æ”¹è¿›çš„é‡è¦æ€§åˆ†æ•°"""
    os.makedirs(save_dir, exist_ok=True)
    
    for method_name, importance in results.items():
        if importance is not None:
            np.save(os.path.join(save_dir, f'{method_name}_importance.npy'), importance)
            
            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            stats = {
                'method': method_name,
                'shape': importance.shape,
                'mean': float(np.mean(importance)),
                'std': float(np.std(importance)),
                'min': float(np.min(importance)),
                'max': float(np.max(importance)),
                'median': float(np.median(importance))
            }
            
            with open(os.path.join(save_dir, f'{method_name}_stats.pkl'), 'wb') as f:
                pickle.dump(stats, f)
    
    print(f"ğŸ’¾ æ”¹è¿›çš„é‡è¦æ€§åˆ†æ•°å·²ä¿å­˜åˆ°: {save_dir}")

def visualize_importance_comparison(results, save_dir='./importance_scores_improved'):
    """å¯è§†åŒ–ä¸åŒæ–¹æ³•çš„é‡è¦æ€§åˆ†æ•°æ¯”è¾ƒ"""
    print("ğŸ“Š åˆ›å»ºé‡è¦æ€§åˆ†æ•°æ¯”è¾ƒå›¾...")
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. ä¸åŒæ–¹æ³•çš„åˆ†æ•°åˆ†å¸ƒ
    ax1 = axes[0, 0]
    for method_name, importance in results.items():
        if importance is not None:
            ax1.hist(importance, bins=30, alpha=0.6, label=method_name)
    ax1.set_xlabel('Importance Score')
    ax1.set_ylabel('Frequency')
    ax1.set_title('Distribution of ROI Importance Scores')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. å‰20ä¸ªæœ€é‡è¦ROIçš„æ¯”è¾ƒ
    ax2 = axes[0, 1]
    top_indices = {}
    for method_name, importance in results.items():
        if importance is not None:
            top_indices[method_name] = np.argsort(importance)[-20:][::-1]
    
    # è®¡ç®—é‡å åº¦
    if len(top_indices) > 1:
        methods = list(top_indices.keys())
        overlap_matrix = np.zeros((len(methods), len(methods)))
        for i, method1 in enumerate(methods):
            for j, method2 in enumerate(methods):
                overlap = len(set(top_indices[method1][:10]) & set(top_indices[method2][:10]))
                overlap_matrix[i, j] = overlap
        
        im = ax2.imshow(overlap_matrix, cmap='Blues', aspect='auto')
        ax2.set_xticks(range(len(methods)))
        ax2.set_yticks(range(len(methods)))
        ax2.set_xticklabels(methods, rotation=45)
        ax2.set_yticklabels(methods)
        ax2.set_title('Top-10 ROI Overlap Between Methods')
        plt.colorbar(im, ax=ax2)
    
    # 3. é‡è¦æ€§åˆ†æ•°æ’åº
    ax3 = axes[1, 0]
    for method_name, importance in results.items():
        if importance is not None:
            sorted_scores = np.sort(importance)[::-1]
            ax3.plot(range(1, len(sorted_scores) + 1), sorted_scores, 
                    label=method_name, linewidth=2)
    ax3.set_xlabel('ROI Rank')
    ax3.set_ylabel('Importance Score')
    ax3.set_title('ROI Importance Scores (Ranked)')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. ç»Ÿè®¡ä¿¡æ¯è¡¨æ ¼
    ax4 = axes[1, 1]
    ax4.axis('off')
    
    info_text = "Importance Score Statistics:\n\n"
    for method_name, importance in results.items():
        if importance is not None:
            info_text += f"{method_name}:\n"
            info_text += f"  Mean: {np.mean(importance):.4f}\n"
            info_text += f"  Std: {np.std(importance):.4f}\n"
            info_text += f"  Max: {np.max(importance):.4f}\n"
            info_text += f"  Min: {np.min(importance):.4f}\n\n"
    
    ax4.text(0.1, 0.9, info_text, transform=ax4.transAxes, fontsize=10,
             verticalalignment='top', fontfamily='monospace')
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'importance_comparison.png'), dpi=300, bbox_inches='tight')
    plt.show()

def main():
    parser = argparse.ArgumentParser(description='æ”¹è¿›çš„ROIé‡è¦æ€§åˆ†æ•°æå–')
    parser.add_argument('--model_path', type=str, default='./model_improved/best_model_fold0.pth',
                       help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--data_path', type=str, default='./data/ABIDE_pcp/cpac/filt_noglobal',
                       help='æ•°æ®è·¯å¾„')
    parser.add_argument('--save_dir', type=str, default='./importance_scores_improved',
                       help='ä¿å­˜é‡è¦æ€§åˆ†æ•°çš„ç›®å½•')
    parser.add_argument('--batch_size', type=int, default=64,
                       help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--device', type=str, default='cpu',
                       help='è®¾å¤‡ (cpu/cuda)')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--indim', type=int, default=200, help='è¾“å…¥ç»´åº¦')
    parser.add_argument('--ratio', type=float, default=0.6, help='poolingæ¯”ä¾‹')
    parser.add_argument('--nclass', type=int, default=2, help='ç±»åˆ«æ•°')
    parser.add_argument('--k', type=int, default=8, help='k (community num)')
    parser.add_argument('--nroi', type=int, default=116, help='ROIæ•° (R)')
    
    args = parser.parse_args()

    # === è‡ªåŠ¨æ¨æ–­ indim, k, nroi ===
    if os.path.exists(args.model_path):
        checkpoint = torch.load(args.model_path, map_location='cpu')
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
        else:
            state_dict = checkpoint
        # å°è¯•ä» Linear å±‚æˆ– conv å±‚æƒé‡è‡ªåŠ¨æ¨æ–­ indim, k, nroi
        indim_auto = None
        k_auto = None
        nroi_auto = None
        # æ¨æ–­ indim
        for k, v in state_dict.items():
            if ('.weight' in k) and (len(v.shape) == 2):
                indim_auto = v.shape[1]
                break
        # æ¨æ–­ k å’Œ nroi (R) ä» n1.0.weight: [k, R]
        if 'n1.0.weight' in state_dict:
            k_auto = state_dict['n1.0.weight'].shape[0]
            nroi_auto = state_dict['n1.0.weight'].shape[1]
        # è‡ªåŠ¨ä¿®æ­£
        if indim_auto is not None:
            if hasattr(args, 'indim') and args.indim != indim_auto:
                print(f"[è‡ªåŠ¨ä¿®æ­£] æ£€æµ‹åˆ°æƒé‡æ–‡ä»¶è¾“å…¥ç‰¹å¾ç»´åº¦ä¸º {indim_auto}ï¼Œè¦†ç›–å‘½ä»¤è¡Œå‚æ•° indim={args.indim}")
            args.indim = indim_auto
            print(f"[è‡ªåŠ¨æ¨æ–­] ä½¿ç”¨æƒé‡æ–‡ä»¶æ¨æ–­çš„è¾“å…¥ç‰¹å¾ç»´åº¦ indim={args.indim}")
        if k_auto is not None:
            if hasattr(args, 'k') and args.k != k_auto:
                print(f"[è‡ªåŠ¨ä¿®æ­£] æ£€æµ‹åˆ°æƒé‡æ–‡ä»¶ k={k_auto}ï¼Œè¦†ç›–å‘½ä»¤è¡Œå‚æ•° k={args.k}")
            args.k = k_auto
            print(f"[è‡ªåŠ¨æ¨æ–­] ä½¿ç”¨æƒé‡æ–‡ä»¶æ¨æ–­çš„ k={args.k}")
        if nroi_auto is not None:
            if hasattr(args, 'nroi') and args.nroi != nroi_auto:
                print(f"[è‡ªåŠ¨ä¿®æ­£] æ£€æµ‹åˆ°æƒé‡æ–‡ä»¶ nroi={nroi_auto}ï¼Œè¦†ç›–å‘½ä»¤è¡Œå‚æ•° nroi={args.nroi}")
            args.nroi = nroi_auto
            print(f"[è‡ªåŠ¨æ¨æ–­] ä½¿ç”¨æƒé‡æ–‡ä»¶æ¨æ–­çš„ nroi={args.nroi}")
        if indim_auto is None or k_auto is None or nroi_auto is None:
            print("[è­¦å‘Š] æœªèƒ½ä»æƒé‡æ–‡ä»¶è‡ªåŠ¨æ¨æ–­å…¨éƒ¨å‚æ•°ï¼Œè¯·ç¡®ä¿ --indim --k --nroi å‚æ•°æ­£ç¡®ï¼")
    else:
        print(f"[è­¦å‘Š] æƒé‡æ–‡ä»¶ {args.model_path} ä¸å­˜åœ¨ï¼Œæ— æ³•è‡ªåŠ¨æ¨æ–­å‚æ•°ã€‚")

    print("ï¿½ï¿½ å¼€å§‹æ”¹è¿›çš„é‡è¦æ€§åˆ†æ•°æå–...")
    
    # 1. åŠ è½½æ•°æ®
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    dataset = ABIDEDataset(args.data_path, 'ABIDE')
    dataset.data.y = dataset.data.y.squeeze()
    dataset.data.x[dataset.data.x == float('inf')] = 0
    
    # è·å–æ•°æ®åˆ†å‰²
    tr_index, val_index, te_index = train_val_test_split(fold=0)
    train_dataset = dataset[tr_index]
    val_dataset = dataset[val_index]
    test_dataset = dataset[te_index]
    
    # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)
    
    # 3. åŠ è½½æ¨¡å‹
    model = Network(args.indim, args.ratio, args.nclass, k=args.k, R=args.nroi)
    if os.path.exists(args.model_path):
        checkpoint = torch.load(args.model_path, map_location='cpu')
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {args.model_path}")
    else:
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
        return
    model = model.to(args.device)
    
    # 4. æå–ä¸åŒæ–¹æ³•çš„é‡è¦æ€§åˆ†æ•°
    results = {}
    
    # ç»Ÿè®¡æ–¹æ³•
    stat_importance, weighted_importance, max_importance = extract_statistical_importance(
        model, test_loader, args.device)
    results['statistical'] = stat_importance
    results['weighted'] = weighted_importance
    results['max_activation'] = max_importance
    
    # æ³¨æ„åŠ›æ–¹æ³•
    attention_importance = extract_attention_based_importance(model, test_loader, args.device)
    results['attention'] = attention_importance
    
    # ç‰¹å¾é‡è¦æ€§
    feature_importance = extract_feature_importance(model, test_loader, args.device)
    results['feature'] = feature_importance
    
    # æ¢¯åº¦æ–¹æ³•
    gradient_importance = extract_gradient_based_importance(model, test_loader, args.device)
    results['gradient'] = gradient_importance
    
    # 5. è®¡ç®—é›†æˆé‡è¦æ€§
    ensemble_importance = calculate_ensemble_importance(results)
    results['ensemble'] = ensemble_importance
    
    # 6. ä¿å­˜ç»“æœ
    save_improved_importance_scores(results, args.save_dir)
    
    # 7. å¯è§†åŒ–æ¯”è¾ƒ
    visualize_importance_comparison(results, args.save_dir)
    
    print("âœ… æ”¹è¿›çš„é‡è¦æ€§åˆ†æ•°æå–å®Œæˆï¼")
    print(f"ğŸ“Š æå–çš„æ–¹æ³•:")
    for method_name, importance in results.items():
        if importance is not None:
            print(f"   - {method_name}: å‡å€¼={np.mean(importance):.4f}, æ ‡å‡†å·®={np.std(importance):.4f}")
    
    # æ¨èä½¿ç”¨çš„æ–¹æ³•
    if ensemble_importance is not None:
        print(f"ğŸ¯ æ¨èä½¿ç”¨é›†æˆæ–¹æ³•çš„é‡è¦æ€§åˆ†æ•°")
        print(f"   æ–‡ä»¶ä½ç½®: {args.save_dir}/ensemble_importance.npy")

    # === è‡ªåŠ¨è¾“å‡ºè„‘åŒºåç§°è¡¨ ===
    import json
    import pandas as pd
    from scipy.stats import spearmanr

    # === è‡ªåŠ¨ç”Ÿæˆ AAL116 ROI åç§°æ˜ å°„ jsonï¼ˆå¦‚ä¸å­˜åœ¨ï¼‰ ===
    aal116_names = [
        "Precentral_L", "Precentral_R", "Frontal_Sup_L", "Frontal_Sup_R",
        "Frontal_Sup_Orb_L", "Frontal_Sup_Orb_R", "Frontal_Mid_L", "Frontal_Mid_R",
        "Frontal_Mid_Orb_L", "Frontal_Mid_Orb_R", "Frontal_Inf_Oper_L", "Frontal_Inf_Oper_R",
        "Frontal_Inf_Tri_L", "Frontal_Inf_Tri_R", "Frontal_Inf_Orb_L", "Frontal_Inf_Orb_R",
        "Rolandic_Oper_L", "Rolandic_Oper_R", "Supp_Motor_Area_L", "Supp_Motor_Area_R",
        "Olfactory_L", "Olfactory_R", "Frontal_Sup_Medial_L", "Frontal_Sup_Medial_R",
        "Frontal_Med_Orb_L", "Frontal_Med_Orb_R", "Rectus_L", "Rectus_R",
        "Insula_L", "Insula_R", "Cingulum_Ant_L", "Cingulum_Ant_R",
        "Cingulum_Mid_L", "Cingulum_Mid_R", "Cingulum_Post_L", "Cingulum_Post_R",
        "Hippocampus_L", "Hippocampus_R", "ParaHippocampal_L", "ParaHippocampal_R",
        "Amygdala_L", "Amygdala_R", "Calcarine_L", "Calcarine_R",
        "Cuneus_L", "Cuneus_R", "Lingual_L", "Lingual_R",
        "Occipital_Sup_L", "Occipital_Sup_R", "Occipital_Mid_L", "Occipital_Mid_R",
        "Occipital_Inf_L", "Occipital_Inf_R", "Fusiform_L", "Fusiform_R",
        "Postcentral_L", "Postcentral_R", "Parietal_Sup_L", "Parietal_Sup_R",
        "Parietal_Inf_L", "Parietal_Inf_R", "SupraMarginal_L", "SupraMarginal_R",
        "Angular_L", "Angular_R", "Precuneus_L", "Precuneus_R",
        "Paracentral_Lobule_L", "Paracentral_Lobule_R", "Caudate_L", "Caudate_R",
        "Putamen_L", "Putamen_R", "Pallidum_L", "Pallidum_R",
        "Thalamus_L", "Thalamus_R", "Heschl_L", "Heschl_R",
        "Temporal_Sup_L", "Temporal_Sup_R", "Temporal_Pole_Sup_L", "Temporal_Pole_Sup_R",
        "Temporal_Mid_L", "Temporal_Mid_R", "Temporal_Pole_Mid_L", "Temporal_Pole_Mid_R",
        "Temporal_Inf_L", "Temporal_Inf_R", "Cerebelum_Crus1_L", "Cerebelum_Crus1_R",
        "Cerebelum_Crus2_L", "Cerebelum_Crus2_R", "Cerebelum_3_L", "Cerebelum_3_R",
        "Cerebelum_4_5_L", "Cerebelum_4_5_R", "Cerebelum_6_L", "Cerebelum_6_R",
        "Cerebelum_7b_L", "Cerebelum_7b_R", "Cerebelum_8_L", "Cerebelum_8_R",
        "Cerebelum_9_L", "Cerebelum_9_R", "Cerebelum_10_L", "Cerebelum_10_R",
        "Vermis_1_2", "Vermis_3", "Vermis_4_5", "Vermis_6", "Vermis_7", "Vermis_8", "Vermis_9", "Vermis_10"
    ]
    aal116_json_path = './aal116_roi_id2name.json'
    if not os.path.exists(aal116_json_path):
        roi2name = {str(i): name for i, name in enumerate(aal116_names)}
        with open(aal116_json_path, 'w') as f:
            json.dump(roi2name, f, indent=2)
        print(f'å·²è‡ªåŠ¨ç”Ÿæˆæ ‡å‡†AAL116è„‘åŒºåç§°æ˜ å°„: {aal116_json_path}')
    # ç”¨AAL116æ ‡å‡†æ˜ å°„
    roi_info_path = aal116_json_path

    ensemble_path = os.path.join(args.save_dir, 'ensemble_importance.npy')
    output_csv = os.path.join(args.save_dir, 'roi_importance_with_name.csv')
    if os.path.exists(ensemble_path) and os.path.exists(roi_info_path):
        importance = np.load(ensemble_path)
        with open(roi_info_path, 'r') as f:
            roi2name = json.load(f)
        df = pd.DataFrame({
            'ROI': np.arange(len(importance)),
            'BrainRegion': [roi2name.get(str(i), f'ROI_{i}') for i in range(len(importance))],
            'Importance': importance
        })
        df = df.sort_values('Importance', ascending=False)
        df['Rank'] = np.arange(1, len(df)+1)
        df.to_csv(output_csv, index=False)
        print(f'å·²ä¿å­˜: {output_csv}')
        print(df.head(10))

    # === è‡ªåŠ¨ä¸ä¸´åºŠæ•°æ®ç›¸å…³æ€§åˆ†æ ===
    clinical_csv = './data/clinical_scores.csv'
    corr_csv = os.path.join(args.save_dir, 'roi_clinical_correlation.csv')
    if os.path.exists(ensemble_path) and os.path.exists(clinical_csv):
        importance = np.load(ensemble_path)
        clinical = pd.read_csv(clinical_csv)
        results = []
        for col in clinical.columns[1:]:
            corr, pval = spearmanr(importance, clinical[col])
            results.append({'ClinicalVar': col, 'SpearmanR': corr, 'PValue': pval})
        df_corr = pd.DataFrame(results).sort_values('PValue')
        df_corr.to_csv(corr_csv, index=False)
        print(f'å·²ä¿å­˜: {corr_csv}')
        print(df_corr.head(5))

    # === è‡ªåŠ¨ç”Ÿæˆå¤šä»»åŠ¡å¯¹æ¯”çƒ­å›¾ ===
    csv_files = glob.glob('./results/*/roi_importance_with_name.csv')
    if len(csv_files) >= 2:
        dfs = []
        task_names = []
        for path in csv_files:
            task = os.path.basename(os.path.dirname(path))
            df = pd.read_csv(path)[['BrainRegion', 'Importance']]
            df = df.rename(columns={'Importance': task})
            dfs.append(df.set_index('BrainRegion'))
            task_names.append(task)
        heat = pd.concat(dfs, axis=1).fillna(0)
        plt.figure(figsize=(8, max(10, len(heat)//3)))
        sns.heatmap(heat, cmap='viridis')
        plt.title('ROI Importance Across Tasks')
        plt.tight_layout()
        multitask_heatmap_path = './results/roi_importance_multitask_heatmap.png'
        plt.savefig(multitask_heatmap_path)
        print(f'å·²ä¿å­˜å¤šä»»åŠ¡å¯¹æ¯”çƒ­å›¾: {multitask_heatmap_path}')

def extract_and_save_importance(args):
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    dataset = PainGraphDataset(args.data_path)
    from torch.utils.data import Subset
    import numpy as np
    import os
    all_task_types = set()
    for i in range(len(dataset)):
        data = dataset.get(i)
        if hasattr(data, 'task_type'):
            all_task_types.add(int(data.task_type[0].item()))
    print(f"æ£€æµ‹åˆ°ä»»åŠ¡ç±»å‹: {sorted(all_task_types)}")

    # å…ˆè¯»å–æ¨¡å‹æƒé‡ï¼Œæ¨æ–­æœŸæœ›çš„è¾“å…¥ç‰¹å¾æ•°
    checkpoint_expected_input_dim = None
    if os.path.exists(args.model_path):
        checkpoint = torch.load(args.model_path, map_location='cpu')
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
        else:
            state_dict = checkpoint
        # æ¨æ–­ indim
        for k, v in state_dict.items():
            if ('.weight' in k) and (len(v.shape) == 2):
                checkpoint_expected_input_dim = v.shape[1]
                break
        print(f"æ¨¡å‹æƒé‡æœŸæœ›è¾“å…¥ç‰¹å¾æ•°: {checkpoint_expected_input_dim}")
    else:
        print(f"âŒ æ¨¡å‹æƒé‡æ–‡ä»¶ {args.model_path} ä¸å­˜åœ¨ï¼Œæ— æ³•æ¨æ–­è¾“å…¥ç‰¹å¾æ•°ã€‚")
        return

    for ttype in sorted(all_task_types):
        if ttype == -1:
            print(f"è·³è¿‡æ— æ•ˆ task_type={ttype}")
            continue
        indices = [i for i in range(len(dataset)) if hasattr(dataset.get(i), 'task_type') and int(dataset.get(i).task_type[0].item()) == ttype]
        if not indices:
            continue
        print(f"\n==== å¤„ç† task_type={ttype} çš„å­é›†ï¼Œå…± {len(indices)} ä¸ªæ ·æœ¬ ====")
        sub_dataset = Subset(dataset, indices)
        sample_data = dataset.get(indices[0])
        if checkpoint_expected_input_dim is not None and sample_data.x.size(1) != checkpoint_expected_input_dim:
            print(f"è·³è¿‡ task_type={ttype}ï¼Œç‰¹å¾æ•° {sample_data.x.size(1)} ä¸æƒé‡æœŸæœ› {checkpoint_expected_input_dim} ä¸ä¸€è‡´")
            continue
        from torch_geometric.loader import DataLoader
        test_loader = DataLoader(sub_dataset, batch_size=args.batch_size, shuffle=False)
        args.indim = sample_data.x.size(1)
        args.nroi = sample_data.x.size(0)
        model = Network(args.indim, args.ratio, args.nclass, k=args.k, R=args.nroi)
        if os.path.exists(args.model_path):
            checkpoint = torch.load(args.model_path, map_location='cpu')
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            else:
                model.load_state_dict(checkpoint)
            print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {args.model_path}")
        else:
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
            continue
        model = model.to(args.device)
        results = {}
        stat_importance, weighted_importance, max_importance = extract_statistical_importance(model, test_loader, args.device)
        results['statistical'] = stat_importance
        results['weighted'] = weighted_importance
        results['max_activation'] = max_importance
        attention_importance = extract_attention_based_importance(model, test_loader, args.device)
        results['attention'] = attention_importance
        feature_importance = extract_feature_importance(model, test_loader, args.device)
        results['feature'] = feature_importance
        gradient_importance = extract_gradient_based_importance(model, test_loader, args.device)
        results['gradient'] = gradient_importance
        ensemble_importance = calculate_ensemble_importance(results)
        results['ensemble'] = ensemble_importance
        save_dir = os.path.join(args.save_dir, f'task{ttype}')
        os.makedirs(save_dir, exist_ok=True)
        save_improved_importance_scores(results, save_dir)
        src_npy = os.path.join(save_dir, 'ensemble_importance.npy')
        dst_npy = os.path.join(save_dir, f'ensemble_importance_task{ttype}.npy')
        if os.path.exists(src_npy):
            import shutil
            shutil.copy(src_npy, dst_npy)
        src_csv = os.path.join(save_dir, 'roi_importance_with_name.csv')
        dst_csv = os.path.join(save_dir, f'roi_importance_task{ttype}.csv')
        if os.path.exists(src_csv):
            shutil.copy(src_csv, dst_csv)
        print(f"å·²ä¿å­˜ task{ttype} çš„é‡è¦æ€§ç»“æœåˆ° {save_dir}")

if __name__ == '__main__':
    # åªå¤„ç† all_graphs ç›®å½•
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--device', type=str, default='cpu')
    parser.add_argument('--batch_size', type=int, default=64)
    parser.add_argument('--ratio', type=float, default=0.6)
    parser.add_argument('--nclass', type=int, default=2)
    parser.add_argument('--indim', type=int, default=200)
    parser.add_argument('--k', type=int, default=8)
    parser.add_argument('--nroi', type=int, default=116)
    parser.add_argument('--model_path', type=str, default='./model/0.pth')
    parser.add_argument('--save_dir', type=str, default='./results/all_graphs')
    args = parser.parse_args()
    args.data_path = 'data/pain_data/all_graphs'
    extract_and_save_importance(args) 