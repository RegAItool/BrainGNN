#!/usr/bin/env python3
"""
ç®€åŒ–çš„é‡è¦æ€§åˆ†æ•°æå–
ä¸“æ³¨äºç»Ÿè®¡æ–¹æ³•æ¥è®¡ç®—ROIé‡è¦æ€§åˆ†æ•°
"""

import torch
import numpy as np
import os
import pickle
import argparse
from torch_geometric.data import DataLoader
from net.braingnn import Network
from imports.ABIDEDataset import ABIDEDataset
from imports.utils import train_val_test_split
import matplotlib.pyplot as plt

def load_trained_model(model_path, args):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    model = Network(args.indim, args.ratio, args.nclass)
    
    if os.path.exists(model_path):
        checkpoint = torch.load(model_path, map_location='cpu')
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")
        return model
    else:
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return None

def extract_improved_importance(model, dataloader, device='cpu'):
    """æå–æ”¹è¿›çš„é‡è¦æ€§åˆ†æ•°"""
    print("ğŸ” æå–æ”¹è¿›çš„é‡è¦æ€§åˆ†æ•°...")
    model.eval()
    all_scores = []
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for batch in dataloader:
            batch = batch.to(device)
            
            output, _, _, score1, score2 = model(batch.x, batch.edge_index, 
                                                batch.batch, batch.edge_attr, batch.pos)
            
            all_scores.append(score1.cpu().numpy())
            pred = output.argmax(dim=1)
            all_predictions.append(pred.cpu().numpy())
            all_labels.append(batch.y.cpu().numpy())
    
    scores = np.concatenate(all_scores, axis=0)
    predictions = np.concatenate(all_predictions, axis=0)
    labels = np.concatenate(all_labels, axis=0)
    
    # åªä½¿ç”¨æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬
    correct_mask = (predictions == labels)
    
    if np.sum(correct_mask) > 0:
        # è®¡ç®—æ¯ä¸ªROIçš„é‡è¦æ€§
        roi_importance = np.mean(scores[correct_mask], axis=0)
        
        # è®¡ç®—ç½®ä¿¡åº¦åŠ æƒçš„é‡è¦æ€§
        confidence = np.max(scores[correct_mask], axis=1)
        weights = confidence / np.sum(confidence)
        weighted_importance = np.average(scores[correct_mask], axis=0, weights=weights)
        
        # è®¡ç®—æœ€å¤§æ¿€æ´»çš„é‡è¦æ€§
        max_importance = np.max(scores[correct_mask], axis=0)
        
        # è®¡ç®—æ ‡å‡†å·®åŠ æƒçš„é‡è¦æ€§
        std_importance = np.std(scores[correct_mask], axis=0)
        
        return roi_importance, weighted_importance, max_importance, std_importance
    else:
        return None, None, None, None

def save_importance_scores(results, save_dir='./importance_scores_improved'):
    """ä¿å­˜é‡è¦æ€§åˆ†æ•°"""
    os.makedirs(save_dir, exist_ok=True)
    
    for method_name, importance in results.items():
        if importance is not None:
            np.save(os.path.join(save_dir, f'{method_name}_importance.npy'), importance)
            
            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            stats = {
                'method': method_name,
                'shape': importance.shape,
                'mean': float(np.mean(importance)),
                'std': float(np.std(importance)),
                'min': float(np.min(importance)),
                'max': float(np.max(importance)),
                'median': float(np.median(importance))
            }
            
            with open(os.path.join(save_dir, f'{method_name}_stats.pkl'), 'wb') as f:
                pickle.dump(stats, f)
    
    print(f"ğŸ’¾ é‡è¦æ€§åˆ†æ•°å·²ä¿å­˜åˆ°: {save_dir}")

def visualize_importance_comparison(results, save_dir='./importance_scores_improved'):
    """å¯è§†åŒ–é‡è¦æ€§åˆ†æ•°æ¯”è¾ƒ"""
    print("ğŸ“Š åˆ›å»ºé‡è¦æ€§åˆ†æ•°æ¯”è¾ƒå›¾...")
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. ä¸åŒæ–¹æ³•çš„åˆ†æ•°åˆ†å¸ƒ
    ax1 = axes[0, 0]
    for method_name, importance in results.items():
        if importance is not None:
            ax1.hist(importance, bins=30, alpha=0.6, label=method_name)
    ax1.set_xlabel('Importance Score')
    ax1.set_ylabel('Frequency')
    ax1.set_title('Distribution of ROI Importance Scores')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. é‡è¦æ€§åˆ†æ•°æ’åº
    ax2 = axes[0, 1]
    for method_name, importance in results.items():
        if importance is not None:
            sorted_scores = np.sort(importance)[::-1]
            ax2.plot(range(1, len(sorted_scores) + 1), sorted_scores, 
                    label=method_name, linewidth=2)
    ax2.set_xlabel('ROI Rank')
    ax2.set_ylabel('Importance Score')
    ax2.set_title('ROI Importance Scores (Ranked)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. å‰20ä¸ªæœ€é‡è¦ROI
    ax3 = axes[1, 0]
    for method_name, importance in results.items():
        if importance is not None:
            top_indices = np.argsort(importance)[-20:][::-1]
            top_scores = importance[top_indices]
            ax3.bar(range(len(top_indices)), top_scores, alpha=0.6, label=method_name)
    ax3.set_xlabel('Top 20 ROIs')
    ax3.set_ylabel('Importance Score')
    ax3.set_title('Top 20 Most Important ROIs')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. ç»Ÿè®¡ä¿¡æ¯è¡¨æ ¼
    ax4 = axes[1, 1]
    ax4.axis('off')
    
    info_text = "Importance Score Statistics:\n\n"
    for method_name, importance in results.items():
        if importance is not None:
            info_text += f"{method_name}:\n"
            info_text += f"  Mean: {np.mean(importance):.6f}\n"
            info_text += f"  Std: {np.std(importance):.6f}\n"
            info_text += f"  Max: {np.max(importance):.6f}\n"
            info_text += f"  Min: {np.min(importance):.6f}\n"
            info_text += f"  Range: {np.max(importance) - np.min(importance):.6f}\n\n"
    
    ax4.text(0.1, 0.9, info_text, transform=ax4.transAxes, fontsize=10,
             verticalalignment='top', fontfamily='monospace')
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'importance_comparison.png'), dpi=300, bbox_inches='tight')
    plt.show()

def main():
    parser = argparse.ArgumentParser(description='ç®€åŒ–çš„é‡è¦æ€§åˆ†æ•°æå–')
    parser.add_argument('--model_path', type=str, default='./model_improved/best_model_fold0.pth',
                       help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--data_path', type=str, default='./data/ABIDE_pcp/cpac/filt_noglobal',
                       help='æ•°æ®è·¯å¾„')
    parser.add_argument('--save_dir', type=str, default='./importance_scores_improved',
                       help='ä¿å­˜é‡è¦æ€§åˆ†æ•°çš„ç›®å½•')
    parser.add_argument('--batch_size', type=int, default=64,
                       help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--device', type=str, default='cpu',
                       help='è®¾å¤‡ (cpu/cuda)')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--indim', type=int, default=200, help='è¾“å…¥ç»´åº¦')
    parser.add_argument('--ratio', type=float, default=0.6, help='poolingæ¯”ä¾‹')
    parser.add_argument('--nclass', type=int, default=2, help='ç±»åˆ«æ•°')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¼€å§‹ç®€åŒ–çš„é‡è¦æ€§åˆ†æ•°æå–...")
    
    # 1. åŠ è½½æ•°æ®
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    dataset = ABIDEDataset(args.data_path, 'ABIDE')
    dataset.data.y = dataset.data.y.squeeze()
    dataset.data.x[dataset.data.x == float('inf')] = 0
    
    # è·å–æ•°æ®åˆ†å‰²
    tr_index, val_index, te_index = train_val_test_split(fold=0)
    train_dataset = dataset[tr_index]
    val_dataset = dataset[val_index]
    test_dataset = dataset[te_index]
    
    # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)
    
    # 3. åŠ è½½æ¨¡å‹
    model = load_trained_model(args.model_path, args)
    if model is None:
        return
    
    model = model.to(args.device)
    
    # 4. æå–é‡è¦æ€§åˆ†æ•°
    results = {}
    
    # ç»Ÿè®¡æ–¹æ³•
    stat_importance, weighted_importance, max_importance, std_importance = extract_improved_importance(
        model, test_loader, args.device)
    results['statistical'] = stat_importance
    results['weighted'] = weighted_importance
    results['max_activation'] = max_importance
    results['std_based'] = std_importance
    
    # 5. ä¿å­˜ç»“æœ
    save_importance_scores(results, args.save_dir)
    
    # 6. å¯è§†åŒ–æ¯”è¾ƒ
    visualize_importance_comparison(results, args.save_dir)
    
    print("âœ… ç®€åŒ–çš„é‡è¦æ€§åˆ†æ•°æå–å®Œæˆï¼")
    print(f"ğŸ“Š æå–çš„æ–¹æ³•:")
    for method_name, importance in results.items():
        if importance is not None:
            print(f"   - {method_name}: å‡å€¼={np.mean(importance):.6f}, æ ‡å‡†å·®={np.std(importance):.6f}")
            print(f"     èŒƒå›´: {np.max(importance) - np.min(importance):.6f}")
    
    # æ¨èä½¿ç”¨çš„æ–¹æ³•
    if stat_importance is not None:
        print(f"ğŸ¯ æ¨èä½¿ç”¨ç»Ÿè®¡æ–¹æ³•çš„é‡è¦æ€§åˆ†æ•°")
        print(f"   æ–‡ä»¶ä½ç½®: {args.save_dir}/statistical_importance.npy")
        
        # æ¯”è¾ƒæ”¹è¿›å‰åçš„æ•ˆæœ
        print(f"\nğŸ“ˆ æ”¹è¿›æ•ˆæœåˆ†æ:")
        print(f"   - æ ‡å‡†å·®: {np.std(stat_importance):.6f}")
        print(f"   - åˆ†æ•°èŒƒå›´: {np.max(stat_importance) - np.min(stat_importance):.6f}")
        print(f"   - å‰10ä¸ªæœ€é‡è¦ROI: {np.argsort(stat_importance)[-10:][::-1]}")

if __name__ == '__main__':
    main() 